{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"14YDYfceHS9ZG3LEEpnvSugSBUHmh511i","timestamp":1585573339000}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j9UIcPrC3i85"},"source":["<font size=20>Laboratory 8: NLP using recurrent neural networks</font>\n"," \n"," \n","In this lab you will:\n","  - understand what a recurrent neural language model (RNNLM) is\n","  - implement an RNNLM using characters as input and a gated architecture (LSTM/GRU/etc.)\n","  - visualize neurons and identify the ones that specialize on different aspects\n","  - generate text using a modern large pretrained language model\n"]},{"cell_type":"markdown","metadata":{"id":"fSplW_0drfq_"},"source":["# Language model"]},{"cell_type":"markdown","metadata":{"id":"XvxYFhZQs8tR"},"source":["## Definition\n","A language model is a system that predicts the next token, given the previous ones:\n","$$P(x_t|x_1,...x_{t-1})$$\n","\n","If we know the intermediate probabilities $P(x_1)$, $P(x_2|x_1)$, $P(x_3|x_1, x_2)$, ..., $P(x_t|x_1,...x_{t-1})$ we can compute the joint probability of the whole sequence $x_1, x_2, ..., x_T$.\n","So, a language model can also be seen as a system that computes the probability of a sequence of tokens (words/characters): \n","\n","$$P(x_1, x_2, x_3, ..., x_T)\n","=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_T|x_1,...,x_{T-1})$$\n","$$=P(x_1) \\prod_{t=1}^{T}P(x_t|x_1,...x_{t-1})$$"]},{"cell_type":"markdown","metadata":{"id":"Rpr2OvR4stcV"},"source":["## Applications\n","\n"," - **speech recognition**: which sentence is more probable?\n"," \n",">> $P(recognize, beach) > P(wreck, a, nice, beach)$\n","\n","- **predictive typing**: what's the next most probable word?\n","\n",">> $P(x_t| I, cannot, wait, to, go, to, the)$\n","\n","- **transfer learning**: training a model to solve this task results in the model learning general language features which can transfer to other tasks"]},{"cell_type":"markdown","metadata":{"id":"-t2W3elGp4Ku"},"source":["## What we'll do today\n"," - train a neural language model using a recurrent cell (LSTM/GRU) and characters as input\n"," - generate novel text using the trained language model \n"," - visualize neuron activations during text generation\n","  \n"," "]},{"cell_type":"markdown","metadata":{"id":"DHECb3YGtWxo"},"source":["## Bird's-eye view of the architecture\n","The network is unrolled for T timesteps. At each timestep t, the network:\n"," - receives:\n"," >- the current token $x_t$ (in our case $x_t$ is a character)\n"," >- the previous hidden state $h_{t-1}$\n"," - produces:\n"," >- the new hidden state $h_t$\n"," >- the probability distribution over the next token  $P(X_{t+1}|x_1,x_2,...,x_{t-1})$\n"," ![char-rnn unrolled](https://i.ibb.co/vQQQ20Y/lstm-predict-next-char.png)\n"," \n"," \n"," The previous correct token (at time $t-1$) becomes the new input at time t.\n"]},{"cell_type":"markdown","metadata":{"id":"Bh15bQHFtdIs"},"source":["## Zoomed-in architecture\n","The network has the following components:\n"," - an **Embedding layer** that converts a character $x_t$ (given as an index) into a character embedding $e_t$:\n"," \n","      $7$ -> Embedding layer -> $[0.12, -0.16, ..., 0.43]$\n","       \n","      The character indices that we feed into the network range from 0 to |V|-1, where |V| is the size of our vocabulary (how many unique characters we represent).\n","       \n"," - an **RNN cell** that computes the new hidden state, based on the current embedding and the old hidden state: $$h_t = f(x_t, h_{t-1};\\theta_{RNN})$$ where $\\theta_{RNN}$ contains the LSTM/GRU parameters\n"," - an **output layer** (linear layer + softmax) that transforms a hidden state vector (of size d) to a vector of probabilities (of size |V|).\n"," \n"," ![Prediction at time step t](https://i.ibb.co/RBcY3WM/rnnlm.png)"]},{"cell_type":"markdown","source":["# Part I: Training a character-based recurrent language model"],"metadata":{"id":"1lyvAVViruHv"}},{"cell_type":"markdown","metadata":{"id":"am6XGjflwWMm"},"source":["## Step 1: Data gathering and preprocessing\n","\n","Download one of the books specified in the `DOWNLOAD_LINKS` dictionary. You can add other books in the dictionary, as long as they are stored as `.txt` files. For more free books you can check out the most popular ones on Gutenberg: https://www.gutenberg.org/browse/scores/top.\n"]},{"cell_type":"code","source":["import zipfile\n","import os\n","from os import path\n","from io import open\n","import glob\n","from collections import Counter\n","from __future__ import unicode_literals, print_function, division\n","from google.colab import files, auth, drive\n","from torch.optim.lr_scheduler import LambdaLR\n","from urllib.request import urlopen\n","from typing import List, Dict, Callable\n","import math\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import RandomSampler, Sampler\n","from torch import nn, Tensor\n","from torch.optim import SGD, Adam\n","import unicodedata\n","import random\n","import string\n","from matplotlib import pyplot as plt\n","\n","DOWNLOAD_LINKS = {\n","    'got': 'https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt',\n","    'eminescu': 'www.gutenberg.org/cache/epub/35323/pg35323.txt',\n","    'alice_in_wonderland': 'https://www.gutenberg.org/files/11/11-0.txt',\n","    'harry_potter_1': \"\"\"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher%27s%20Stone.txt\"\"\",\n","    'harry_potter_2': \"\"\"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\"\"\",\n","    'witcher_last_wish': \"\"\"https://raw.githubusercontent.com/dworschak/Witcher/master/RESSOURCES/_books/text/C%20-%20The%20Last%20Wish.txt\"\"\",\n","    'c++': 'https://cs.stanford.edu/people/karpathy/char-rnn/linux.txt'\n","}\n","\n","# TODO: change book title here \n","BOOK_TITLE = 'witcher_last_wish'\n","\n","# download book and save it to the /tmp/ dir in your Drive\n","link = r\"{}\".format(DOWNLOAD_LINKS[BOOK_TITLE])\n","save_path = r\"/tmp/{}.txt\".format(BOOK_TITLE)\n","print(\"Downloading {} from {} and saving it to {}\".format(BOOK_TITLE, link, save_path))\n","\n","!wget --no-check-certificate {link} -O {save_path}"],"metadata":{"id":"gKpKWX7RVzOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5dXK0t9ZI12X"},"source":["def readfile(filepath: str) -> str:\n","    \"\"\"\n","    Reads file and returns its content as a string.\n","    \"\"\"\n","    #response = urlopen(url)\n","    #body = response.read().decode('utf-8')\n","    with open(filepath, \"r\") as f:\n","        body = f.read()\n","    \n","    return body.encode('ascii', 'ignore').decode(\"utf-8\")\n","\n","# read book contents\n","text = readfile(save_path)\n","\n","#make sure the text is properly read\n","print(\"Size of text: %d characters\" % len(text))\n","print(\"First 300 characters: \\n\", text[0:300])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_path"],"metadata":{"id":"xzQDyAwygqY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91evcTYfdaTO"},"source":["### Text representation\n","Let us define V = {'a', 'b', 'c', ..., '!'}  as the set of all unique characters that appear in our text. This is called the **vocabulary**.\n","\n","Each character is stored as an index that ranges from 0 to |V|-1 (e.g. 'a'->0, 'b'->1 etc.)\n","\n","The Vocabulary class below creates the **character->index** and **index->character** mappings, given a text."]},{"cell_type":"code","metadata":{"id":"C_Zw3ZPPZj6J"},"source":["class Vocabulary:\n","    \"\"\"\n","    Helper class that maps characters to unique indices and the other way around\n","    \"\"\"\n","    def __init__(self, text: str):\n","        # PAD is a special character for padding shorter sequences \n","        # in a mini-batch\n","        characters_set = set([\"<PAD>\"]) \n","        characters_set.update(text)\n","        \n","        self.char_to_idx = {char:idx for (idx, char) \n","                            in enumerate(characters_set)}\n","        self.idx_to_char = {idx:char for (idx, char) \n","                            in enumerate(characters_set)}\n","   \n","    def size(self):\n","        return len(self.char_to_idx)\n","      \n","    def __str__(self):\n","        return str(self.char_to_idx)\n","\n","vocab = Vocabulary(text)\n","print(\"Vocabulary size: \", vocab.size())\n","print(\"Vocabulary: \\n\", vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTbI_rNLh-la"},"source":["**TODO**: Write the text_to_tensor and tensor_to_text functions below. Check that a text is correctly encoded and decoded back."]},{"cell_type":"code","metadata":{"id":"dPMrXc11iL7r"},"source":["def text_to_tensor(text: str, vocab: Vocabulary) -> torch.LongTensor:\n","    \"\"\"\n","    Convert a string to a Tensor with corresponding character indices\n","    e.g. \"We have\" -> [48, 13,  2, 66, 56, 31, 13]\n","    \"\"\"\n","    text_indices = [vocab.char_to_idx[c] for c in text]\n","    \n","    return torch.tensor(text_indices)\n","  \n","def tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> str:\n","    \"\"\"\n","    Convert a Tensor of character indices to its string representation\n","    e.g. [48, 13,  2, 66, 56, 31, 13] -> \"We have\"\n","    \"\"\"\n","    return \"\".join(vocab.idx_to_char[idx.item()] for idx in x)\n","\n","def batch_tensor_to_text(x: torch.LongTensor, vocab: Vocabulary) -> List[str]:\n","    \"\"\"\n","    The batched version of tensor_to_text\n","    E.g. [[2, 1, 3, 0, 0], [3, 1, 20]] -> [bac, cat]\n","    :param x: Tensor of size (batch_size x time_steps)\n","    :return: a list of corresponding strings \n","    \"\"\"\n","    assert len(x.size()) == 2, \"wrong number of dimensions (should be 2)\"\n","    outputs = []\n","    for batch_idx in range(len(x)):\n","        outputs.append(tensor_to_text(x[batch_idx], vocab))\n","            \n","    return outputs \n","  \n","# check that a random text is correctly converted to numbers and \n","# back to text\n","random_text = \"I have apples\"\n","encoded_text = text_to_tensor(random_text, vocab)\n","decoded_text = tensor_to_text(encoded_text, vocab)\n","assert random_text == decoded_text, \"Not the same text as the original\"\n","\n","# convert input text to numbers\n","data = text_to_tensor(text, vocab)\n","print(\"Size of input tensor is: \", data.size(0))\n","\n","# setup device (CPU/GPU)\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","else:\n","    device = torch.device('cpu')\n","\n","# move tensor to CUDA if available\n","data = data.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2Pe_OOcj3Uh"},"source":["### Splitting large sequence into mini-batches\n","\n","The input tensor is *very long*, so we cannot unroll the recurrent network for that many timesteps. In practice, we limit the maximum number of timesteps to an order of hundreds (100-200). We also use a batch of several examples instead of just one example, so the mini-batch input to our RNN looks like this:\n","\n","![batch_input](https://i.ibb.co/ZzLd4fQ/embedding-input.png)"]},{"cell_type":"markdown","metadata":{"id":"l25U_Ih2yJhj"},"source":["Assume that our input document has 250 characters, and we use batches of size 2 and 15 timesteps. \n","\n","We split the document into chunks of 2x15 and discard the extra characters:\n","\n","    250 / 30 = 8 => we throw the last 10 characters\n","  \n","The remaining 240 characters will be split into 8 batches, each of size 2 x 15.  As we can see in the picture below the first batch will consist of the 1st and 9th row, the second batch will consist of the 2nd and 10th row and so on:\n"," - batch 1 inputs: | I've had the c|, |here his ears h|\n"," - batch 2 inputs: |old in me too, |, |ad been. \"Two e| \n","\n","At each timestep, the network predicts the next character, so the output sequence is just the \n","input sequence shifted by one position to the right:\n"," - batch 1 outputs: |I've had the co|, |ere his ears ha|\n"," - batch 2 outputs: |ld in me too, l|, |d been. \"Two ea|\n","\n","The RNN is stateful, which means the initial hidden state of the RNN for batch **k** is the last hidden state from batch **k-1**. In our example, the first hidden state when processing *|old in me too, |* is the last hidden state obtained from *| I've had the c|*."]},{"cell_type":"markdown","metadata":{"id":"XAK_SwuuY_qA"},"source":["**Question**: Why didn't we create the batches this way:\n"," - batch 1: (1st row, 2nd row)\n"," - batch 2: (3rd row, 4th row)\n"," - batch 3: (5th row, 6th row)\n"," - ..."]},{"cell_type":"markdown","metadata":{"id":"8ToGu-UIx834"},"source":["![batched_document](https://i.ibb.co/G26pPw8/batched-input.png)"]},{"cell_type":"markdown","metadata":{"id":"dc3t8TK_IY-w"},"source":["**TODO**: Write the make_batches function below"]},{"cell_type":"code","metadata":{"id":"n33PHl3i5w1d"},"source":["def make_batches(x: torch.LongTensor, batch_size: int, max_len: int) -> \\\n","            (List[torch.LongTensor], List[torch.LongTensor]):\n","    \"\"\"\n","    Returns a tuple of Tensors of size (batch_size x max_len), that\n","    represent the inputs and outputs for each feedforward call\n","    through the network\n","    :param x: Tensor of size textsize\n","    :param batch_size: how many sequences to process at a time\n","    :param max_len: maximum length of sequence example\n","    \"\"\"\n","    # ensure full batches (trim extra-content)\n","    text_size = len(x)\n","    num_iterations = text_size // (batch_size * max_len)\n","    assert num_iterations > 0, \"large batch_size/max_len or short text\"\n","    trimmed_size = num_iterations * batch_size * max_len\n","    x_trim = x[:trimmed_size]   \n","    # batch_size x (max_len x num_iterations)\n","    x_trim = x_trim.reshape((batch_size, -1))\n","       \n","    # we'll use T=0 (shifted upwards) as the last output\n","    # batch_size\n","    first_column = x_trim[:, 0].detach()\n","    # batch_size\n","    # [14, 23, 54] => [23, 54, 54]\n","    first_column[:-1] = first_column[1:].clone()\n","    \n","    x_batches = [] #inputs: tensors of size batch_size x max_len\n","    y_batches = [] #outputs: tensors of size batch_size x max_len \n","    for iter in range(0, (num_iterations-1) * max_len, max_len):\n","        # tensors of size (batch_size x max_len)\n","        current_x = x_trim[:, iter: iter + max_len]\n","        current_y = torch.zeros_like(current_x)\n","        # output are inputs shifted to the left by one\n","        current_y[:,:] = x_trim[:, iter + 1: iter + max_len + 1]\n","        yield current_x, current_y\n","        \n","    iter += max_len\n","    current_x = x_trim[:, iter: iter + max_len]\n","    current_y = torch.zeros_like(current_x)\n","    current_y[:,:-1] = x_trim[:, iter + 1: iter + max_len]\n","    current_y[:,-1] = first_column\n","    \n","    yield current_x, current_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTphHQmUIN3Z"},"source":["#split corpus into 80% training/20% validation set\n","train_dev_cutoff = int(len(data) * 0.8)\n","train_data, dev_data = data[:train_dev_cutoff], data[train_dev_cutoff:]\n","print(\"Training set size = \", len(train_data))\n","print(\"Evaluation set size = \", len(dev_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4ox3AeELbJh"},"source":["def print_sep(s: str, max_string_len: int):\n","    num_rows = len(s) // max_string_len\n","    for i in range(0, len(s), max_string_len):\n","        print(\"|\" + s[i:i+max_string_len] + \"|\")\n","\n","print(\"First 250 characters: \\n\")\n","print_sep(tensor_to_text(train_data[:250], vocab), 15)\n","\n","# CHECKPOINT: verify that the batches are correctly split\n","it = make_batches(train_data[0:250], 2, 15)\n","xb, yb = next(it)\n","print(\"First batch input: \", batch_tensor_to_text(xb, vocab))\n","print(\"First batch output: \", batch_tensor_to_text(yb, vocab))\n","\n","xb2, yb2 = next(it)\n","print(\"Second batch input: \", batch_tensor_to_text(xb2, vocab))\n","print(\"Second batch output: \", batch_tensor_to_text(yb2, vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hGdX7mZq7oC"},"source":["### Character embedding\n","Each character is stored as an index (from 0 to |V|-1, where |V| is the number of unique characters - in our case 88). Each character is represented as a dense vector:  [0.2, 0.13, -0.1, ..., 0.11]\n","  \n","This vector is called an *embedding* and is stored as a row in an embedding matrix W. The matrix W has size |V| x d, where d is the dimension of each *embedding*.\n","\n","The **Embedding** layer in PyTorch is a wrapper over W (https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) that receives a scalar **i** and returns the **i-th** row of W. \n","   \n","\n"]},{"cell_type":"code","metadata":{"id":"N1Md4CfDtHZ1"},"source":["char_idx = torch.LongTensor([7]) # we want character at index 7\n","\n","# create an Embedding layer that outputs vectors of size 100; \n","# behind the scenes, this is just a weight matrix of size vocab_size x 100\n","emb_layer = nn.Embedding(vocab.size(), 100)\n","\n","# this is the actual weight matrix \n","W = emb_layer.weight \n","print(\"Embedding matrix size: \", W.size())\n","\n","# the embedding layer receives an index and returns the corresponding row at \n","# that index in the weight matrix\n","char_embedding = emb_layer(char_idx).squeeze()\n","print(\"Character embedding size: \", char_embedding.size())\n","\n","# this layer returns 7th row in the matrix as output\n","# let's check this\n","if char_embedding.equal(W[char_idx].squeeze()):\n","    print(\"Same embedding\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5M7pME0_pBsr"},"source":["## Step 2: Setup architecture\n","In this section, you are going to prepare all the components of a recurrent language model: \n"," - embedding layer\n"," - recurrent layer (there are two variants of RNN classes in Pytorch: X and XCell, where X is the name of the architecture (LSTM/GRU/etc.) The Cell modules can only process one input at a time, instead of T timesteps. You should use the Cell modules for now)\n"," - output layer\n"," - loss function\n"," \n"," All the neccessary classes are found in the PyTorch documentation: https://pytorch.org/docs/stable/index.html"]},{"cell_type":"code","metadata":{"id":"G1IoPAjE3nop"},"source":["############################## PARAMETERS ######################################\n","_hyperparameters_dict = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 20,\n","    \"max_len\": 250,\n","    \"embedding_size\": 256, \n","    \"rnn_size\": 1024,\n","    \"learning_algo\": \"adam\",\n","    \"learning_rate\": 0.001,\n","    \"max_grad_norm\": 5.0\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ao80RD9P-t_6"},"source":["class RNNLM(nn.Module):\n","    def __init__(self, vocab_size: int, char_embedding_size: int, \n","                 rnn_size: int):\n","        super().__init__()\n","        #TODO\n","        self.vocab_size = vocab_size\n","        self.char_embedding_size = char_embedding_size\n","        self.rnn_size = rnn_size\n","        self.dropout = nn.Dropout(p=0.5)\n","        \n","        # TODO: instantiate Modules with the correct arguments\n","        self.embedding = nn.Embedding(num_embeddings = vocab_size, \n","                                      embedding_dim = char_embedding_size)\n","\n","        self.rnn_cell = nn.GRUCell(input_size = char_embedding_size,\n","                                   hidden_size = rnn_size)\n","        self.logits = nn.Linear(in_features=rnn_size, out_features=vocab_size)\n","        self.softmax = nn.Softmax(dim = 2)\n","        \n","        self.loss = nn.CrossEntropyLoss()\n","    \n","    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n","        \"\"\"\n","        Computes loss for a batch of sequences. The sequence loss is the \n","        average of the individual losses at each timestep. The batch loss is\n","        the average of sequence losses across all batches.\n","\n","        :param logits: unnormalized probabilities for T timesteps, size\n","                       batch_size x max_timesteps x vocab_size\n","        :param y: ground truth values (index of correct characters), size\n","                  batch_size x max_timesteps\n","        :returns: loss as a scalar\n","        \"\"\"\n","        #TODO: \n","        #logits: B x T x vocab_size\n","        #        B x T\n","        \n","        #cross entropy: B x vocab_size x T\n","        #               B x T\n","        #vision: B x num_classes\n","        #        B\n","        return self.loss(logits.permute(0, 2, 1), y)\n","        \n","    \n","    def get_logits(self, hidden_states: torch.FloatTensor, \n","                   temperature: float = 1.0):\n","        \"\"\"\n","        Computes the unnormalized probabilities from hidden states. Optionally\n","        divide logits by a temperature, in order to influence predictions at \n","        test time (https://www.quora.com/What-is-Temperature-in-LSTM)\n","        \n","        :param hidden_states: tensor of size batch_size x timesteps x rnn_size\n","        :param temperature: coefficient that scales outputs before turning them \n","        to probabilities. A low temperature (0.1) results in more conservative \n","        predictions, while a higher temperature (0.9) results in more diverse\n","        predictions\n","        \n","        :return: tensor of size batch_size x timesteps x vocab_size\n","        \"\"\"\n","        return self.logits(self.dropout(hidden_states)) / temperature\n","        \n","    def forward(self, x: torch.LongTensor, \n","                hidden_start: torch.FloatTensor = None) -> torch.FloatTensor:\n","        \"\"\"\n","        Computes the hidden states for the current batch (x, y). \n","        :param x: input of size batch_size x max_len\n","        :param hidden_start: hidden state at time step t = 0, \n","                             size batch_size x rnn_size\n","        :return: hidden states at all timesteps, \n","                 size batch_size x timesteps x rnn_size\n","        \"\"\"\n","        max_len = x.size(1)\n","        \n","        #batch_size x max_len x embedding_dim\n","        x_embedded = self.embedding(x)\n","        \n","        #compute hidden states and logits for each time step\n","        hidden_states_list = []\n","        prev_hidden = hidden_start\n","        for t in range(max_len):\n","            hidden_state = self.rnn_cell(x_embedded[:,t,:], prev_hidden)\n","            hidden_states_list.append(hidden_state)\n","            prev_hidden = hidden_state\n","        \n","        # return all hidden states as a Tensor\n","        #batch_size x max_len x rnn_size\n","        hidden_states = torch.stack(hidden_states_list, dim=1)\n","        \n","        return hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kASwYd2K62RE"},"source":["#instantiate the RNNLM module\n","network = RNNLM(vocab.size(), \n","            _hyperparameters_dict['embedding_size'], \n","            _hyperparameters_dict['rnn_size'])\n","\n","# move network to GPU if available\n","network = network.to(device)\n","\n","optimizer = Adam(params = network.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LA1XhLQ2grD"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# CHECKPOINT: make sure you understand each parameter size\n","print(\"Neural network parameters: \")\n","for param_name, param in network.named_parameters():\n","    print(\"\\t\" + param_name, \" size: \", param.size())\n","\n","print(\"#parameters: \", count_parameters(network))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PW9cU8DKgi36"},"source":["# CHECKPOINT: make sure you can feedforward and backpropagate through network\n","iterator = make_batches(train_data, 32, 20)\n","xb, yb = next(iterator)\n","hidden_start = torch.zeros(_hyperparameters_dict[\"batch_size\"],\n","                            _hyperparameters_dict[\"rnn_size\"]).to(device)\n","hidden_states = network(xb, hidden_start)\n","logits = network.get_logits(hidden_states)\n","loss = network.get_loss(logits, yb)\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sc9hgcB42oTX"},"source":["#train the network for 60 iterations and save save hidden states (one slice of \n","#the batch) every 30 iterations\n","iterator = make_batches(train_data, 32, 50)\n","\n","hidden_states_it = [] #list of tensors of size timesteps x rnn_size\n","prev_hidden = torch.zeros(_hyperparameters_dict[\"batch_size\"],\n","                           _hyperparameters_dict[\"rnn_size\"]).to(device)\n","for it in range(61):\n","    xb, yb = next(iterator)\n","    \n","    #gradients are set to zero every new batch\n","    optimizer.zero_grad()\n","    \n","    #feedforward\n","    hidden_states = network(xb, prev_hidden)\n","    logits = network.get_logits(hidden_states)\n","    loss = network.get_loss(logits, yb)\n","    \n","    #backpropagation -> compute gradient of loss with respect to all weights\n","    loss.backward()\n","    \n","    #clip gradients if they get have norm > 5.0\n","    torch.nn.utils.clip_grad_norm_(list(network.parameters()), 5.0)\n","    \n","    #update weights \n","    optimizer.step()\n","    \n","    #the hidden states from iteration it should no longer be linked to \n","    #the hidden states from iteration (it+1)\n","    hidden_states.detach_()\n","    \n","    if it % 30 == 0:\n","        print(\"Iteration %d, loss = %f\" %(it, loss))\n","        hidden_states_it.append(hidden_states[0,:,:].cpu())\n","        \n","    prev_hidden = hidden_states[:,-1,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xa5ltOn5BlGN"},"source":["#plot hidden states after 0, 30 and 60 iterations\n","#the horizontal axis shows the timesteps\n","#the vertical axis shows the neurons of the hidden state \n","#you can see in the 3 figures how one hidden state evolves\n","#through the iterations \n","h1, h2, h3 = hidden_states_it[0].numpy().transpose(),\\\n","             hidden_states_it[1].numpy().transpose(),\\\n","             hidden_states_it[2].numpy().transpose()\n","\n","fig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n","ax1.pcolor(h1, cmap = 'Reds', alpha = 0.5, vmin = -1.0, vmax = 1.0)\n","ax1.set_title(\"Hidden states iteration 0\")\n","\n","ax2.pcolor(h2, cmap = 'Reds', alpha = 0.5, vmin = -1.0, vmax = 1.0)\n","ax2.set_title(\"Hidden states iteration 30\")\n","\n","ax3.pcolor(h3, cmap = 'Reds', alpha = 0.5, vmin = -1.0, vmax = 1.0)\n","ax3.set_title(\"Hidden states iteration 60\")\n","\n","plt.subplots_adjust(top = 2.0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcvXKTXXu_B_"},"source":["## Step 3: Setup training/evaluation loop\n","\n","Write the `train_epoch` and `eval_epoch` methods in the `Trainer` class below:"]},{"cell_type":"code","metadata":{"id":"OIE5iLptuEkN"},"source":["class Trainer:\n","    def __init__(self, model: nn.Module, \n","                 train_data: torch.LongTensor, \n","                 dev_data: torch.LongTensor,\n","                 vocab: Vocabulary, \n","                 hyperparams: Dict):\n","        self.model = model\n","        self.train_data = train_data\n","        self.dev_data = dev_data\n","        self.vocab = vocab\n","        if hyperparams['learning_algo'] == 'adam':\n","            self.optimizer = Adam(params = self.model.parameters(),\n","                                  lr = hyperparams['learning_rate'])\n","        else:\n","            self.optimizer = SGD(params = self.model.parameters(), \n","                                 lr = hyperparams['learning_rate'])\n","        self.num_epochs = hyperparams['num_epochs']\n","        self.max_len = hyperparams['max_len']\n","        self.batch_size = hyperparams['batch_size']\n","        self.rnn_size = hyperparams['rnn_size']\n","        self.max_grad_norm = hyperparams['max_grad_norm']\n","        \n","        #number of characters in training/dev data\n","        self.train_size = len(train_data)\n","        self.dev_size = len(dev_data)\n","        \n","        #number of sequences (X, Y) used for training\n","        self.num_train_examples = \\\n","          self.train_size // (self.batch_size * self.max_len) * self.batch_size\n","        \n","        \n","    def train_epoch(self, epoch_num: int) -> float:\n","        \"\"\"\n","        Compute the loss on the training set\n","        :param epoch_num: number of current epoch\n","        \"\"\"\n","        self.model.train()\n","        epoch_loss = 0.0\n","        hidden_start = torch.zeros(self.batch_size, self.rnn_size).to(device)\n","        for batch_num, (x, y) in enumerate(make_batches(self.train_data, \n","                                                       self.batch_size, \n","                                                       self.max_len)):\n","            # reset gradients\n","            self.optimizer.zero_grad()\n","          \n","            # compute hidden states\n","            # batch x timesteps x hidden_size\n","            hidden_states = self.model(x, hidden_start)\n","            \n","            # compute unnormalized probabilities\n","            # batch x timesteps x vocab_size\n","            logits = self.model.get_logits(hidden_states)\n","            \n","            # compute loss\n","            # scalar\n","            batch_loss = self.model.get_loss(logits, y)\n","            epoch_loss += batch_loss.item()\n","                       \n","            # backpropagation (gradient of loss wrt parameters)\n","            batch_loss.backward()\n","            \n","            # clip gradients if they get too large\n","            torch.nn.utils.clip_grad_norm_(list(self.model.parameters()), \n","                                           self.max_grad_norm)\n","            \n","            # update parameters\n","            self.optimizer.step()\n","            \n","            # we use a stateful RNN, which means the first hidden state for the\n","            # next batch is the last hidden state of the current batch\n","            hidden_states.detach_()\n","            hidden_start = hidden_states[:,-1,:] #TODO add comment\n","            \n","            if batch_num % 100 == 0:\n","                print(\"epoch %d, %d/%d examples, batch loss = %f\"\n","                      % (epoch_num, (batch_num + 1) * self.batch_size, \n","                         self.num_train_examples, batch_loss.item()))\n","        epoch_loss /= (batch_num + 1)\n","        \n","        return epoch_loss\n","\n","    def eval_epoch(self, epoch_num: int) -> float:\n","        \"\"\"\n","        Compute the loss on the validation set\n","        :param epoch_num: number of current epoch\n","        \"\"\"\n","        epoch_loss = 0.0\n","        self.model.eval()\n","        hidden_start = torch.zeros(self.batch_size, self.rnn_size).to(device)\n","        with torch.no_grad():\n","            for batch_num, (x, y) in enumerate(make_batches(self.dev_data, \n","                                                            self.batch_size, \n","                                                            self.max_len)):    \n","                #batch x timesteps x hidden_size\n","                hidden_states = self.model(x, hidden_start)\n","            \n","                #batch x timesteps x vocab_size\n","                logits = self.model.get_logits(hidden_states)\n","            \n","                batch_loss = self.model.get_loss(logits, y)\n","                epoch_loss += batch_loss.item()\n","                \n","                # we use a stateful RNN, which means the first hidden state for \n","                # the next batch is the last hidden state of the current batch\n","                hidden_states.detach_()\n","                hidden_start = hidden_states[:,-1,:]\n","                \n","            epoch_loss /= (batch_num + 1)\n","        \n","        return epoch_loss    \n","            \n","    def train(self) -> Dict:\n","        train_losses, dev_losses = [], []\n","        for epoch in range(self.num_epochs):\n","            epoch_train_loss = self.train_epoch(epoch)\n","            epoch_dev_loss = self.eval_epoch(epoch)\n","            train_losses.append(epoch_train_loss)\n","            dev_losses.append(epoch_dev_loss)\n","        return {\"train_losses\": train_losses,\n","                \"dev_losses\": dev_losses}\n","\n","def plot_losses(metrics: Dict):\n","    \"\"\"\n","    Plots training/validation losses.\n","    :param metrics: dictionar\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(metrics['train_losses'], c='b', label='Train')\n","    plt.plot(metrics['dev_losses'], c='g', label='Valid')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Iteration')\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cQYCgLMuRru"},"source":["#train network for some epoch\n","trainer = Trainer(network, train_data, dev_data, vocab, _hyperparameters_dict)\n","metrics = trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDqKrvkk0RXc"},"source":["#plot training and validations losses each epoch\n","plot_losses(metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeLKcia5oJRm"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Czm9jicy89D"},"source":["## Step 4: Generate text \n","\n","We can use the trained language model to generate novel text that is written in the style of the text it was trained on. At each timestep, the network outputs a probability distribution over characters.  We can either take the character with maximum probability, or we can sample from this distribution.\n","\n","Write the `greedy_strategy` and `generate_text_from` functions below:"]},{"cell_type":"code","metadata":{"id":"_ozUZDOI0hsH"},"source":["def greedy_strategy(probabilities: torch.FloatTensor) -> int:\n","    \"\"\"\n","    Given a Tensor with probabilities, this function returns the character\n","    index with maximum probability\n","    :params probabilities: Tensor of size vocabulary_size\n","    \"\"\"\n","    return torch.argmax(probabilities).item()\n","      \n","def sample_strategy(probabilities: torch.FloatTensor) -> int:\n","    \"\"\"\n","    Given a Tensor with probabilities (categorical distribution), this function\n","    returns the character index of a sampled element\n","    :params probabilities: Tensor of size vocabulary_size\n","    \"\"\"\n","    return torch.multinomial(probabilities, 1).item()\n","\n","class TextGenerator:\n","    def __init__(self, model, vocab, params):\n","        self.vocab = vocab\n","        self.model = model\n","        self.params = params\n","        self.softmax = nn.Softmax(dim=0)\n","                \n","    def generate_text_from(self, starting_text: str, \n","                           timesteps: int, \n","                           strategy: Callable[..., int], \n","                           temperature: float = 1.0) -> (str, torch.FloatTensor):\n","        \"\"\"\n","        This method generates text from an RNN based on the input text. It also\n","        returns the hidden states associated with each generated character\n","        :param starting_text: text that we condition on\n","        :param timesteps: how many characters to generate\n","        :param strategy: either greedy_strategy or sample_strategy\n","        :param temperature: scale the outputs before softmax-ing\n","        \"\"\"\n","        # convert text string to Tensor of character indices\n","        # string_len\n","        starting_data = text_to_tensor(starting_text, self.vocab).to(device)\n","        \n","        # 1 x string_len\n","        starting_data = starting_data.unsqueeze(0)\n","              \n","        # we first need to feed the starting text into the RNN characters by \n","        # character; we then use the last hidden state as a starting point to \n","        # generate new text\n","        # 1 x string_len x hidden_size\n","        with torch.no_grad():\n","            hidden_states = self.model(starting_data)\n","        \n","        # 1 x hidden_size\n","        prev_hidden = hidden_states[:, -1, :]\n","        \n","        # 1 x vocab_size\n","        logits = self.model.get_logits(prev_hidden, temperature)\n","        \n","        # vocab_size\n","        prev_probs = self.softmax(logits.squeeze())\n","                \n","        # start generating text\n","        # select most probable character from probability distribution\n","        generated_text = \"\"          \n","        hidden_states_list = []\n","        with torch.no_grad(): #no gradients computed inside this context\n","            for t in range(timesteps):\n","                # store hidden state\n","                hidden_states_list.append(prev_hidden.squeeze())\n","\n","                # select character index according to strategy\n","                top_char_idx = strategy(prev_probs)\n","\n","                # add character to already generated text\n","                generated_text += self.vocab.idx_to_char[top_char_idx]\n","\n","                # prepare new input\n","                # top_char_idx is a number, but the network needs a Tensor\n","                # of size (batch_size x timesteps) as input.\n","                # During training, we used multiple rows at a time, \n","                # but now we only generate one stream of text, so \n","                # batch_size = 1. We also generate characters one at a\n","                # time, so timesteps = 1\n","                # 1 x 1 (batch_size = 1, timesteps = 1)\n","                current_input = torch.LongTensor([top_char_idx]).to(device)\n","                current_input = current_input.reshape(1, 1)\n","                \n","                hidden_states = self.model(current_input, prev_hidden)\n","                prev_hidden = hidden_states[:, -1, :]\n","                prev_logits = self.model.get_logits(prev_hidden, temperature)\n","                prev_probs = self.softmax(prev_logits.squeeze())\n","        \n","        return generated_text, hidden_states_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqivHODsj4ej"},"source":["#CHECKPOINT: try greedy and sampling strategies (with different temperatures)\n","#and inspect the difference in outputs\n","generator = TextGenerator(network, vocab, _hyperparameters_dict)\n","#starting_text = \"int main(\"\n","starting_text = \"Gerald entered the village \"\n","\n","greedy_text, hidden_states_greedy = generator.generate_text_from(\n","            starting_text = starting_text, \n","            timesteps = 200, \n","            strategy = greedy_strategy,\n","            temperature = 1.0)\n","\n","low_temp_text, hidden_states_low = generator.generate_text_from(\n","            starting_text = starting_text, \n","            timesteps = 200, \n","            strategy = sample_strategy,\n","            temperature = 1.0)\n","print(\"Greedy text: \", greedy_text)\n","print(\"Low temperature text: \", low_temp_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_s6onwS2GdTj"},"source":["## Step 5: Visualize cells\n","\n","We're going to visualize how the i-th neuron inside a hidden state changes during text generation, similarly to http://karpathy.github.io/2015/05/21/rnn-effectiveness .\n","\n","\n","We need to train a recurrent neural language model on `C++` code. Retrain the language model on the `c++` file from the `DOWNLOAD_LINKS` dictionary (or download a larger code file from [here](https://drive.google.com/drive/folders/1vsik60ql2HHLJcMTOhp02mNq0YuTmURc?usp=sharing) or somewhere else). The file contains many snippets of `C++` code stitched together. \n","\n","After training a language model on this data, generate some code that contains parantheses (). Call `plot_string` for different values of `neuron_index` and see if you can identify a neuron that 'activates' when opening and when closing a paranthesis."]},{"cell_type":"code","metadata":{"id":"ou7oW__6GjdW"},"source":["def plot_string(generated_text: str, hidden_states: torch.FloatTensor, \n","                neuron_index: int, title=None):\n","    \"\"\"\n","    :param generated_text: string generated by RNN\n","    :param hidden_states: hidden states from which string was generated,\n","                          size len(generated_text) x hidden_size\n","    :param neuron_index: index in {0, 1, ... , hidden_size - 1}\n","    Plots a grid with characters and a corresponding color (based on\n","    the value of the neuron)\n","    \"\"\"\n","    assert len(generated_text) == len(hidden_states)\n","    \n","    #select activations at position neuron_index\n","    activations = hidden_states[:, neuron_index].cpu()\n","    \n","    width = 15\n","    height = len(generated_text) // width + 1\n","    missing_width = width - len(generated_text) % width\n","    generated_text += ' ' * missing_width\n","    activations = torch.cat((activations, \n","                             torch.zeros(missing_width)))\n","    activations = activations.reshape(height, width).numpy()\n","    \n","    plt.figure(figsize=(width * 0.5, height * 0.5))\n","    plt.title(title)\n","    plt.imshow(activations, interpolation='none', cmap='Reds', alpha=0.5,\n","              vmin=-1.0, vmax=1.0)\n","    plt.axis('off')\n","  \n","    for row_id in range(height):\n","        for col_id in range(width):\n","            plt.text(col_id - 0.2, row_id + 0.2, \n","                     generated_text[row_id * width + col_id],\n","                     color = 'k', fontsize = 12)\n","    plt.show()\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qltbcGY9VnDb"},"source":["hidden_tensor = torch.stack(hidden_states_low)\n","plot_string(low_temp_text, hidden_tensor, 7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part II: Generating text using a large pretrained language model\n","\n","We have trained a small recurrent character-based language model on a small amount of data and got decent generation results. \n","\n","The modern language models in NLP have several features:\n"," - they are based on a model different than recurrent neural networks, called Transformer\n"," - they are far larger than our toy model (the largest model to date is Megatron-Turing and it has 530 billion parameters)\n"," - they are trained on large data (hundreds of billions of tokens)\n"],"metadata":{"id":"fUWEjM1mnkIV"}},{"cell_type":"markdown","source":["We now test an already pretrained large language model and inspect its generation capabilities. The model is called GPT-Neo, which was trained to replicate the performance of GPT3. Due to memory constraints, we will use a smaller variant of 125M parameters, called [GPT-Neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125m)."],"metadata":{"id":"blmYDeYupmdk"}},{"cell_type":"code","source":["!pip install transformers\n","from transformers import pipeline\n","MODEL_NAME = 'gptneo'\n","MODELS = {\n","    'gptneo': 'EleutherAI/gpt-neo-125M',\n","    'gpt2': 'gpt2'\n","}\n","\n","#generator = pipeline('text-generation', model='EleutherAI/gpt-neo-125M')\n","generator = pipeline('text-generation', model=MODELS[MODEL_NAME]"],"metadata":{"id":"KD6K_sFinmdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use sampling\n","generator(\"Gerald has entered the village with\", do_sample=True, max_length=100)"],"metadata":{"id":"bsqH6htYpuGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#greedy decoding\n","generator(\"Gerald has entered the village with\", do_sample=False, max_length=100)"],"metadata":{"id":"IdjL_dpTvb3x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"T5b4CPOflg7v"}},{"cell_type":"markdown","source":["# Part III: Training a character-based Transformer language model"],"metadata":{"id":"9Klc6A1Q3lhT"}},{"cell_type":"markdown","metadata":{"id":"Ovl8uW4s3z1E"},"source":["## Setup architecture\n","We keep the previous components of the *recurrent language model* (embedding layer, output layer, loss function), but instead use the [Transformer encoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html?highlight=transformerencoder#torch.nn.TransformerEncoder) block instead of an RNN.\n","\n"]},{"cell_type":"code","metadata":{"id":"A_DssRGL3z1F"},"source":["############################## PARAMETERS ######################################\n","_hyperparameters_dict = {\n","    \"batch_size\": 32,\n","    \"num_epochs\": 20,\n","    \"max_len\": 250,\n","    \"embedding_size\": 512, \n","    \"learning_algo\": \"adam\",\n","    \"learning_rate\": 1,\n","    \"max_grad_norm\": 5.0\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1DWeW343z1G"},"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        Args:\n","            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n","        \"\"\"\n","        x = x + self.pe[:x.size(0)]\n","        return self.dropout(x)\n","\n","class TransformerLM(nn.Module):\n","    def __init__(self, vocab_size: int, char_embedding_size: int):        \n","        super().__init__()\n","        #TODO\n","        self.vocab_size = vocab_size\n","        self.char_embedding_size = char_embedding_size\n","        self.dropout = nn.Dropout(p=0.5)\n","        \n","        # positional encoding layer initialized\n","        self.pos_encoder = PositionalEncoding(char_embedding_size, 0.5)\n","\n","        # TODO: instantiate Modules with the correct arguments\n","        self.embedding = nn.Embedding(num_embeddings = vocab_size, \n","                                      embedding_dim = char_embedding_size)\n","\n","        #self.rnn_cell = nn.GRUCell(input_size = char_embedding_size,\n","        #                           hidden_size = rnn_size)\n","        # instantiate encoder layer\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=char_embedding_size, \n","            nhead=4,\n","            batch_first=True\n","        )\n","\n","        self.encoder = nn.TransformerEncoder(\n","            encoder_layer, \n","            num_layers=6\n","        )\n","\n","        # instantiate Transformer encoder with N=1 encoder layers\n","        self.logits = nn.Linear(\n","            in_features=char_embedding_size, \n","            out_features=vocab_size\n","        )\n","        self.softmax = nn.Softmax(dim = 2)\n","        \n","        self.loss = nn.CrossEntropyLoss()\n","    \n","    def get_loss(self, logits: torch.FloatTensor, y: torch.FloatTensor):\n","        \"\"\"\n","        Computes loss for a batch of sequences. The sequence loss is the \n","        average of the individual losses at each timestep. The batch loss is\n","        the average of sequence losses across all batches.\n","\n","        :param logits: unnormalized probabilities for T timesteps, size\n","                       batch_size x max_timesteps x vocab_size\n","        :param y: ground truth values (index of correct characters), size\n","                  batch_size x max_timesteps\n","        :returns: loss as a scalar\n","        \"\"\"\n","        #TODO: \n","        #logits: B x T x vocab_size\n","        #        B x T\n","        \n","        #cross entropy: B x vocab_size x T\n","        #               B x T\n","        #vision: B x num_classes\n","        #        B\n","        return self.loss(logits.permute(0, 2, 1), y)\n","        \n","    \n","    def get_logits(self, hidden_states: torch.FloatTensor, \n","                   temperature: float = 1.0):\n","        \"\"\"\n","        Computes the unnormalized probabilities from hidden states. Optionally\n","        divide logits by a temperature, in order to influence predictions at \n","        test time (https://www.quora.com/What-is-Temperature-in-LSTM)\n","        \n","        :param hidden_states: tensor of size batch_size x timesteps x rnn_size\n","        :param temperature: coefficient that scales outputs before turning them \n","        to probabilities. A low temperature (0.1) results in more conservative \n","        predictions, while a higher temperature (0.9) results in more diverse\n","        predictions\n","        \n","        :return: tensor of size batch_size x timesteps x vocab_size\n","        \"\"\"\n","        return self.logits(hidden_states) / temperature\n","        \n","    def forward(self, x: torch.LongTensor, \n","                hidden_start: torch.FloatTensor = None) -> torch.FloatTensor:\n","        \"\"\"\n","        Computes the hidden states for the current batch (x, y). \n","        :param x: input of size batch_size x max_len\n","        :param hidden_start: hidden state at time step t = 0, \n","                             size batch_size x rnn_size\n","        :return: hidden states at all timesteps, \n","                 size batch_size x timesteps x rnn_size\n","        \"\"\"\n","        max_len = x.size(1)\n","        \n","        # batch_size x max_len x embedding_dim\n","        # multiply embeddings by sqrt(embedding_size)\n","        x_embedded = self.embedding(x) * math.sqrt(self.char_embedding_size)\n","\n","        x_embedded = self.pos_encoder(x_embedded)\n","        \n","        # compute hidden states\n","        # batch_size x max_len x embedding_dim\n","        hidden_states = self.encoder(x_embedded, is_causal=True)\n","        \n","        return hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzrxzL8m3z1H"},"source":["#instantiate the TransformerLM module\n","model = TransformerLM(vocab.size(), _hyperparameters_dict['embedding_size'])\n","\n","# move network to GPU if available\n","model = model.to(device)\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# CHECKPOINT: make sure you understand each parameter size\n","print(\"TransformerLM parameters: \")\n","for param_name, param in model.named_parameters():\n","    print(\"\\t\" + param_name, \" size: \", param.size())\n","print(\"#parameters: \", count_parameters(model))\n","\n","optimizer = Adam(params = model.parameters(), lr=0.01)\n","\n","# test: make sure you can feedforward and backpropagate through network\n","iterator = make_batches(train_data, 32, 20)\n","xb, yb = next(iterator)\n","\n","hidden_states = model(xb)\n","logits = model.get_logits(hidden_states)\n","loss = model.get_loss(logits, yb)\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpUC2A7dGmGn"},"source":["## Setup training/evaluation loop\n","\n","Write the `train_epoch` and `eval_epoch` methods in the `Trainer` class below:"]},{"cell_type":"code","metadata":{"id":"wkPniDaGGmGt"},"source":["def rate(step, model_size, factor, warmup):\n","    \"\"\"\n","    we have to default the step to 1 for LambdaLR function\n","    to avoid zero raising to negative power.\n","    \"\"\"\n","    if step == 0:\n","        step = 1\n","    return factor * (\n","        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n","    )\n","\n","class Trainer:\n","    def __init__(self, model: nn.Module, \n","                 train_data: torch.LongTensor, \n","                 dev_data: torch.LongTensor,\n","                 vocab: Vocabulary, \n","                 hyperparams: Dict):\n","        self.model = model\n","        self.train_data = train_data\n","        self.dev_data = dev_data\n","        self.vocab = vocab\n","        if hyperparams['learning_algo'] == 'adam':\n","            self.optimizer = Adam(params = self.model.parameters(),\n","                                  lr = hyperparams['learning_rate'])\n","        else:\n","            self.optimizer = SGD(params = self.model.parameters(), \n","                                 lr = hyperparams['learning_rate'])\n","        self.lr_scheduler = LambdaLR(\n","            optimizer=self.optimizer, lr_lambda=lambda step: rate(step, 512, 1, 4000)\n","        )\n","        self.num_epochs = hyperparams['num_epochs']\n","        self.max_len = hyperparams['max_len']\n","        self.batch_size = hyperparams['batch_size']\n","        self.max_grad_norm = hyperparams['max_grad_norm']\n","        \n","        #number of characters in training/dev data\n","        self.train_size = len(train_data)\n","        self.dev_size = len(dev_data)\n","        \n","        #number of sequences (X, Y) used for training\n","        self.num_train_examples = \\\n","          self.train_size // (self.batch_size * self.max_len) * self.batch_size\n","        \n","        \n","    def train_epoch(self, epoch_num: int) -> float:\n","        \"\"\"\n","        Compute the loss on the training set\n","        :param epoch_num: number of current epoch\n","        \"\"\"\n","        self.model.train()\n","        epoch_loss = 0.0\n","        for batch_num, (x, y) in enumerate(make_batches(self.train_data, \n","                                                       self.batch_size, \n","                                                       self.max_len)):\n","            # reset gradients\n","            self.optimizer.zero_grad()\n","          \n","            # compute hidden states\n","            # batch x timesteps x hidden_size\n","            hidden_states = self.model(x)\n","            \n","            # compute unnormalized probabilities\n","            # batch x timesteps x vocab_size\n","            logits = self.model.get_logits(hidden_states)\n","            \n","            # compute loss\n","            # scalar\n","            batch_loss = self.model.get_loss(logits, y)\n","            epoch_loss += batch_loss.item()\n","                       \n","            # backpropagation (gradient of loss wrt parameters)\n","            batch_loss.backward()\n","            \n","            # clip gradients if they get too large\n","            torch.nn.utils.clip_grad_norm_(list(self.model.parameters()), \n","                                           self.max_grad_norm)\n","            \n","            # update parameters\n","            self.optimizer.step()\n","\n","            # update lr scheduler\n","            self.lr_scheduler.step()\n","            \n","            if batch_num % 100 == 0:\n","                print(\"epoch %d, %d/%d examples, batch loss = %f\"\n","                      % (epoch_num, (batch_num + 1) * self.batch_size, \n","                         self.num_train_examples, batch_loss.item()))\n","        epoch_loss /= (batch_num + 1)\n","        \n","        return epoch_loss\n","\n","    def eval_epoch(self, epoch_num: int) -> float:\n","        \"\"\"\n","        Compute the loss on the validation set\n","        :param epoch_num: number of current epoch\n","        \"\"\"\n","        epoch_loss = 0.0\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch_num, (x, y) in enumerate(make_batches(self.dev_data, \n","                                                            self.batch_size, \n","                                                            self.max_len)):    \n","                #batch x timesteps x hidden_size\n","                hidden_states = self.model(x)\n","            \n","                #batch x timesteps x vocab_size\n","                logits = self.model.get_logits(hidden_states)\n","            \n","                batch_loss = self.model.get_loss(logits, y)\n","                epoch_loss += batch_loss.item()\n","\n","            epoch_loss /= (batch_num + 1)\n","        \n","        return epoch_loss    \n","            \n","    def train(self) -> Dict:\n","        train_losses, dev_losses = [], []\n","        for epoch in range(self.num_epochs):\n","            epoch_train_loss = self.train_epoch(epoch)\n","            epoch_dev_loss = self.eval_epoch(epoch)\n","            train_losses.append(epoch_train_loss)\n","            dev_losses.append(epoch_dev_loss)\n","        return {\"train_losses\": train_losses,\n","                \"dev_losses\": dev_losses}\n","\n","def plot_losses(metrics: Dict):\n","    \"\"\"\n","    Plots training/validation losses.\n","    :param metrics: dictionar\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(metrics['train_losses'], c='b', label='Train')\n","    plt.plot(metrics['dev_losses'], c='g', label='Valid')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Iteration')\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train network for some epochs\n","trainer = Trainer(model, train_data, dev_data, vocab, _hyperparameters_dict)\n","metrics = trainer.train()"],"metadata":{"id":"qFgQ3We_IlA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot training and validations losses each epoch\n","plot_losses(metrics)\n","\n","# 1.0\n","# epoch 0, 32/1568 examples, batch loss = 4.423364\n","# epoch 1, 32/1568 examples, batch loss = 3.274754\n","# epoch 2, 32/1568 examples, batch loss = 2.815885\n","# epoch 3, 32/1568 examples, batch loss = 2.642738\n","# epoch 4, 32/1568 examples, batch loss = 2.572660\n","# epoch 5, 32/1568 examples, batch loss = 2.529776\n","# epoch 6, 32/1568 examples, batch loss = 2.498164\n","# epoch 7, 32/1568 examples, batch loss = 2.479875\n","# epoch 8, 32/1568 examples, batch loss = 2.468683\n","# epoch 9, 32/1568 examples, batch loss = 2.452371\n","# epoch 10, 32/1568 examples, batch loss = 2.443314\n","# epoch 11, 32/1568 examples, batch loss = 2.441196\n","# epoch 12, 32/1568 examples, batch loss = 2.433023\n","# epoch 13, 32/1568 examples, batch loss = 2.424824\n","# epoch 14, 32/1568 examples, batch loss = 2.420776\n","# epoch 15, 32/1568 examples, batch loss = 2.413759\n","# epoch 16, 32/1568 examples, batch loss = 2.408727\n","# epoch 17, 32/1568 examples, batch loss = 2.406193\n","# epoch 18, 32/1568 examples, batch loss = 2.400785\n","# epoch 19, 32/1568 examples, batch loss = 2.393851\n","\n","# 0.1\n","# epoch 0, 32/1568 examples, batch loss = 4.573735\n","# epoch 1, 32/1568 examples, batch loss = 4.366054\n","# epoch 2, 32/1568 examples, batch loss = 3.800495\n","# epoch 3, 32/1568 examples, batch loss = 3.310174\n","# epoch 4, 32/1568 examples, batch loss = 3.080372\n","# epoch 5, 32/1568 examples, batch loss = 2.925282\n","# epoch 6, 32/1568 examples, batch loss = 2.804048\n","# epoch 7, 32/1568 examples, batch loss = 2.736379\n","# epoch 8, 32/1568 examples, batch loss = 2.684438\n","# epoch 9, 32/1568 examples, batch loss = 2.641169\n","# epoch 10, 32/1568 examples, batch loss = 2.615259\n","# epoch 11, 32/1568 examples, batch loss = 2.590087\n","# epoch 12, 32/1568 examples, batch loss = 2.570265\n","# epoch 13, 32/1568 examples, batch loss = 2.554052\n","# epoch 14, 32/1568 examples, batch loss = 2.537857\n","# epoch 15, 32/1568 examples, batch loss = 2.520575\n","# epoch 16, 32/1568 examples, batch loss = 2.516292\n","# epoch 17, 32/1568 examples, batch loss = 2.503919\n","# epoch 18, 32/1568 examples, batch loss = 2.492633\n","# epoch 19, 32/1568 examples, batch loss = 2.488435\n","\n","# 0.01\n","#0, 32/1568 examples, batch loss = 4.449667\n","# epoch 1, 32/1568 examples, batch loss = 4.419884\n","# epoch 2, 32/1568 examples, batch loss = 4.355954\n","# epoch 3, 32/1568 examples, batch loss = 4.260401\n","# epoch 4, 32/1568 examples, batch loss = 4.123610\n","# epoch 5, 32/1568 examples, batch loss = 3.962368\n","# epoch 6, 32/1568 examples, batch loss = 3.781730\n","# epoch 7, 32/1568 examples, batch loss = 3.610585\n","# epoch 8, 32/1568 examples, batch loss = 3.457080\n","# epoch 9, 32/1568 examples, batch loss = 3.329399\n","# epoch 10, 32/1568 examples, batch loss = 3.230541\n","# epoch 11, 32/1568 examples, batch loss = 3.161726\n","# epoch 12, 32/1568 examples, batch loss = 3.104134\n","# epoch 13, 32/1568 examples, batch loss = 3.049460\n","# epoch 14, 32/1568 examples, batch loss = 3.003369\n","# epoch 15, 32/1568 examples, batch loss = 2.957371\n","# epoch 16, 32/1568 examples, batch loss = 2.915635\n","# epoch 17, 32/1568 examples, batch loss = 2.873770\n","# epoch 18, 32/1568 examples, batch loss = 2.841464\n","# epoch 19, 32/1568 examples, batch loss = 2.811225"],"metadata":{"id":"NiG_Hml6Iqcs"},"execution_count":null,"outputs":[]}]}