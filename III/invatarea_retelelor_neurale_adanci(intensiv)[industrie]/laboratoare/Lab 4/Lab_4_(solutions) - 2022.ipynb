{"cells":[{"cell_type":"markdown","metadata":{"id":"NjZTLbz4Vx4c"},"source":["<font size=25>Laboratory 4 summary</font>\n","\n","In this lab you will gain debugging experience by solving the most typical deep learning bugs. \n","\n","There are 10 exercises, each one with a corresponding cell. Run the cell, inspect the error and fix the code. \n","\n","Tips:\n"," - the bugs can be fixed by editing one or two lines of code  \n"," - some code in the sections must not be modified and is clearly delimited with comments\n"," - try not to inspect other exercises while solving the current one"]},{"cell_type":"markdown","metadata":{"id":"HUoo-4E12opk"},"source":["# **Bugs everywhere**"]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import os\n","import torch\n","import random\n","from typing import Iterator, List, Callable, Tuple\n","from functools import partial\n","import warnings\n","from math import *\n","import zipfile\n","from tqdm import tqdm\n","from PIL import Image\n","\n","# Sklearn\n","from sklearn.datasets import load_digits\n","# Numpy\n","import numpy as np\n","# Pandas\n","import pandas as pd\n","\n","# PyTorch packages\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.utils.data import RandomSampler, Sampler\n","from torchvision import transforms, utils, datasets\n","from torchvision.transforms import ToTensor, ToPILImage\n","import torch.nn as nn\n","\n","# matplotlib\n","from matplotlib import rc, cm\n","rc('animation', html='jshtml')\n","import matplotlib.pyplot as plt\n","from mpl_toolkits import mplot3d\n","import matplotlib.animation as animation\n","%matplotlib notebook\n","#warnings.filterwarnings(\"ignore\")\n","plt.ion()   # interactive mode"],"metadata":{"id":"8vvjpuXSFmFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678437310464,"user_tz":-120,"elapsed":5434,"user":{"displayName":"Florin Brad","userId":"06472899283008656976"}},"outputId":"ddd01dad-c7e4-426f-cde1-8e3e4d041730"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.pyplot._IonContext at 0x7f943064a640>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"BsINHO_NwDv5"},"source":["## Exercise 1: Getting started\n"]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY MODEL INSTANTIATION \n","model = MLP(input_size=100, hidden_size=256, activation_fn=nn.ReLU())\n","\n","# issue: wrong input shape\n","#x = torch.rand(32, 200)\n","\n","# solution\n","x = torch.rand(32, 100)\n","\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"k1BvB8TwEPzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Getting in shape"],"metadata":{"id":"uHW1o4myBc4m"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        # self.output_layer = nn.Linear(2*hidden_size, 2)\n","        # issue 1: output_layer input size doesn't match hidden_layer output\n","        # size => modify self.output_layer dimensions\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY MODEL INSTANTIATION \n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n","\n","# download MNIST dataset\n","mnist_trainset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","\n","# select 10th example\n","x, l = mnist_trainset[10]\n","\n","# issue 2: shape of x is 1x3x32x32, but the network takes a tensor of shape 784\n","# solution: resize x before feeding it to the network\n","# when -1 is passed as an argument, the actual dimension is inferred from the\n","# remaining dimensions\n","# 1x3x32x32 -> 1x(784)\n","x = x.view(1, -1) \n","y = model(x)\n","assert y.shape[0] == 1 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"3GCJ9iNHNFIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3: It's the little things"],"metadata":{"id":"3beV3q_MBf4u"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        \n","        # issue 1: activation_fn is accidentally applied over x\n","        # instead of h => shape mismatch\n","        # h = self.activation_fn(x)\n","\n","        # solution: \n","        h = self.activation_fn(h)\n","        \n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# issue 2: activation_fn is accidentally passed as a function pointer,\n","# instead of object\n","# model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU)\n","\n","# solution:\n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n","\n","x = torch.rand(32, 784)\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"ijj1R6SXP-fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: No one left behind"],"metadata":{"id":"QAJtebM_Bjck"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.batch_size = batch_size\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        # issue: for batch_size=32, we always expected tensors of size \n","        # 32 x 1 x 28 x 28. However, the last batch in the dataset was 16,\n","        # so the view function below reshaped the 16 x 1 x 28 x 28 tensor to\n","        # 32 x (1 x 14 x 14) = 32 x 784. This new dimension (784) turned out\n","        # to be incompatible with the first layer of the network\n","        \n","        # input x has shape: batch_size x 1 x 28 x 28 \n","        # we resize it to:   batch_size x 784\n","        # x = x.view(self.batch_size, -1)\n","\n","        # solution 2:\n","        # We extract the actual batch size first and then resize the tensor\n","        # accordingly.\n","        current_batch_size = x.shape[0]\n","        x = x.view(current_batch_size, -1)\n","\n","        # solution 3:\n","        # We resize the tensor based on the 1st input layer dimension\n","        x = x.view(-1, 784)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# instantiate model\n","BATCH_SIZE=32\n","model = MLP(\n","    input_size=784, \n","    hidden_size=256, \n","    activation_fn=nn.ReLU(), \n","    batch_size=BATCH_SIZE\n",")\n","\n","# instantiate MNIST dataset\n","val_dataset = datasets.MNIST(\n","    root='./data', \n","    train=False, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","# issue: dataset has 10000 examples, so the last batch has only 16 elements\n","# instead of 32. The forward method in the MLP class has a bug.\n","# It accidentally reshapes the last batch Tensor from 16 x 1 x 28 x 28\n","# to 32 x (1 x 14 x 14) = 32 x 196. Therefore, the new tensor shape is\n","# incompatible with the first layer of the model.\n","# val_dataloader = DataLoader(\n","#     val_dataset,\n","#     batch_size=BATCH_SIZE,\n","#     shuffle=True\n","# )\n","\n","# solution 1: set the drop_last argument in the DataLoader to True. This\n","# drops the last batch (which may have a different shape) so that all\n","# the batches have the same number of examples\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    drop_last=False\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in val_dataloader:\n","    # batch_size x 2\n","    out = model(batch_images)\n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(val_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"id":"cbXjL4t37_JZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 5: Left to their own devices\n"],"metadata":{"id":"BvVmqB1tHSF4"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","        self.batch_size = batch_size\n","\n","    def forward(self, x):\n","        # issue: Tensor.to() is not an in-place operation, so tensor x\n","        # remains on CPU\n","        # -\n","        # move input data to GPU (if available)\n","        # x.to(self.device)\n","\n","        # solution\n","        x = x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY DEVICE TENSOR BELOW\n","###################################################################################\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"device = \", device)\n","###################################################################################\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32,\n","    device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation, it recursively \n","# processes parameters inside your nn.Module)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    # issue: Tensor.to() is not an in-place operation (though Model.to() is)\n","    # therefore batch_labels remains on CPU\n","    # -\n","    # move labels to GPU (if available)\n","    # batch_labels.to(device)\n","\n","    # solution:\n","    #batch_labels=batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855,"referenced_widgets":["82ee47d85da04df6bbec5bc2dbbc0f83","eb23187ee7f84a8caf9de227f3c2a5c7","7791e791e71a4624abc0ee5f3b993837","37e20b1658cf471eb5e9e3eb60d8f714","fe9857e31d144161ab3dfa0da694cfb2","70242058f5f445958cfc2f5cc97f9fc2","539d613253f046d9ba3d5a6cca4e7336","78c4fcd456d043cabaf5cbbe2759162d","c250ff8c5f6741f7b8aeee35bb875ed0","0d6cff1d27b24792807bf5ddd1b2b98a","5b5b1b3d8df54e93b536623d8ba108b2","5625556804e1402ea35bad12a50602a7","b79f4a06afae43bfb8d52b8164550dcf","f693688ab84e4409864468718e1f65ea","1fcad33212074d7190cee61b0acf06a1","2932938c95c14112bac48bd31b2cb20a","072664088ed34e049ccc45b1543235a3","46f5a980615c4cd388def319f7f3a721","8d2ccc5e585c42db99e35f967fb3ea10","8b107775747e4ebfb7b2e7f17817631e","b805a7f4aae64af4acea9567d2508e8a","dc81171d82814f8cb5447878eb0ff5e2","8883d141ffe74da8b8c13c1e58047513","355b3351b29d4961a4e5cdaacf262734","1e483e2e124040fc8a035ea3179b189f","a783b52295e2472e85da514318004f59","8d71cb6a6c334edaaed30e4314db24f1","a394c7b2e9d54998b47008230cca4e19","00721e39470c4c87843c62304fcf5eaa","fda2cb5f22654485ae6825284bb502da","3e4bb6fe52b94972a4e764cd3cba2861","bb79ebc20ff3436ab81285dd53bf0f7d","c2394553a493485198b4b25e4dbdaf41","96e2e4cbff4f4b8ba62f8353f2a9c6f2","b23c1c008bbf41f59e0e5605132fe900","9a14590276dc4c5ca20dbceaacccbd72","e00bb6d2dc014523a1df7681cd07dbc1","a1918aae919e4493bf4880368a56e5ff","40b9c5938cf84c728b746e90602fd7e8","8671ca14c40e43fe85e76733243cf809","79048debe5f040978bdd7d3a95a36478","ac3308818c5842d4887aa7e0db7cac7f","f7be7a221d0a463abb1498227798797f","33d595b780c34d358cf151b4c33a46f6"]},"id":"bymjthPlMBY3","executionInfo":{"status":"error","timestamp":1646774491904,"user_tz":-120,"elapsed":17631,"user":{"displayName":"Florin Brad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06472899283008656976"}},"outputId":"eea0ab2b-404a-4ab5-8c7f-8fff412021b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device =  cuda\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82ee47d85da04df6bbec5bc2dbbc0f83","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5625556804e1402ea35bad12a50602a7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8883d141ffe74da8b8c13c1e58047513","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96e2e4cbff4f4b8ba62f8353f2a9c6f2","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","train dataset size =  60000\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c6698a8d7da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_crt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_nll_loss_forward)"]}]},{"cell_type":"markdown","source":["## Exercise 6: Not exactly my type"],"metadata":{"id":"6xEcE46Y9q74"}},{"cell_type":"markdown","source":["### Task I"],"metadata":{"id":"ZRzK_UOV_ZT5"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","        self.batch_size = batch_size\n","\n","    def forward(self, x):\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32,\n","    device=device\n",")\n","\n","# issue: the dataset was downloaded but the input was not converted to Tensors,\n","# which leads to a type mismatch during feedforward\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    batch_labels\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"pUDPmShp_Zzi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task II"],"metadata":{"id":"PTfHxrID_aji"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 64\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=64, hidden_size=256, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# load the 1797 images from the Digits dataset:\n","# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n","# Images are grayscale digits from 0 to 9, stored as arrays of size 64 (8x8). \n","# Both images and labels are stored are NumPy arrays, so we need to convert \n","# them to Tensors.\n","x = load_digits()\n","\n","# issue: NumPy arrays are 64bit floating point type, so torch.tensor() converts \n","# them to Tensors of type Double (torch.float64) by default. However,\n","# the default type of Tensors is torch.float32, so when we instantiated the\n","# model, its parameters have type torch.float32 as well. This leads to a \n","# type mismatch when feeding a torch.float64 input to the first layer.\n","# -\n","# 1797 x 64, 1797\n","#images, labels = torch.tensor(x.data), torch.tensor(x.target)\n","\n","# solution 1: specify dtype argument in the torch.tensor() constructor\n","images, labels = torch.tensor(x.data, dtype=torch.float32), torch.tensor(x.target)\n","\n","# solution 2: use torch.to(dtype=torch.float32) to convert to Float type\n","images, labels = torch.tensor(x.data), torch.tensor(x.target)\n","images = images.to(dtype=torch.float32)\n","\n","# solution 3: call .float() on tensor to convert it to Float type\n","images, labels = torch.tensor(x.data), torch.tensor(x.target)\n","images = images.float()\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (1797 in our case)\n","train_dataset = TensorDataset(images, labels)\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"WDdeFJtSW0ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Out of bounds\n","The [Wheat Seeds](https://archive.ics.uci.edu/ml/datasets/seeds) dataset ([Kaggle link](https://www.kaggle.com/jmcaro/wheat-seedsuci)) is a classification task with 3 classes, which contains 209 examples. Each example contains 7 geometrical properties of wheat seeds belonging to 3 varieties of wheat. \n","\n","**Hint 1:** When training on GPUs, CUDA errors may be less helpful. Usually, errors such as \"`RuntimeError: CUDA error: device-side assert triggered`\" indicate a problem with an index, which may be too large. To get a more accurate error message, move the model and dataset to CPU, check the error again and try to fix it.\n","\n","**Hint 2:** After fixing the code responsible for a CUDA error, you may still encounter the error when running on GPU. Try restarting the Colab Notebook (`Runtime` -> `Restart runtime`) and run the cells again.\n"],"metadata":{"id":"1WRHhMa4nX8R"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 3)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 7\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","\n","# if you encounter a vague CUDA error message, move the operations to CPU then\n","# run the code again. The error message is usually more helpful.\n","#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device = torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# download Wheat Seeds dataset\n","!wget --no-check-certificate \\\n","https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n","-O /tmp/wheat.csv\n","\n","# read Wheat Seeds dataset from csv\n","# Dataset has 209 examples. Each example has 7 attributes (features).\n","# It's a classification task with 3 classes (1, 2 and 3)\n","data = pd.read_csv(\"/tmp/wheat.csv\")\n","\n","# issue: the labels read from the CSV are 1, 2 and 3. However, the scores that\n","# the model outputs in the `out` tensor are indexed from 0 to 2, leading to\n","# an index error when trying to access out[:, 3]\n","#x = torch.tensor(data.values, dtype=torch.float32)\n","#data, labels = x[:,:-1], x[:,-1].long()\n","\n","# solution: subtract 1 from the `labels` tensor\n","x = torch.tensor(data.values, dtype=torch.float32)\n","data, labels = x[:,:-1], x[:,-1].long()-1\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (209 in our case)\n","validation_dataset = TensorDataset(data, labels)\n","\n","# instantiate dataloader\n","validation_dataloader = DataLoader(\n","    validation_dataset,\n","    batch_size=BATCH_SIZE\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in validation_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # feedforward\n","    # batch_size x 3\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(validation_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":623},"id":"xZP5yCbeo5Pd","executionInfo":{"status":"error","timestamp":1646749072291,"user_tz":-120,"elapsed":1650,"user":{"displayName":"Florin Brad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06472899283008656976"}},"outputId":"8128f677-4124-4cce-8783-da527013af84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device =  cpu\n","--2022-03-08 14:17:50--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9301 (9.1K) [text/plain]\n","Saving to: ‘/tmp/wheat.csv’\n","\n","/tmp/wheat.csv      100%[===================>]   9.08K  --.-KB/s    in 0s      \n","\n","2022-03-08 14:17:51 (33.8 MB/s) - ‘/tmp/wheat.csv’ saved [9301/9301]\n","\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-98b3636302cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_crt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 3 is out of bounds."]}]},{"cell_type":"markdown","source":["## Exercise 8: I have no memory of that\n","\n","**Hint 1:** The error will appear after ~1 epoch\n","\n","**Hint 2:** You do NOT need to modify the model's size to fix the memory bug\n","\n","**Hint 3:** After getting the error message, you have to restart the machine:\n","  - restart Colab: `Runtime` -> `Restart runtime`\n","  - run the cell that imports packages \n","  - run the cell below"],"metadata":{"id":"4v5D0k3MHsW0"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size_1: int, \n","                 hidden_size_2: int, \n","                 hidden_size_3: int, \n","                 hidden_size_4: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size_1 = hidden_size_1\n","        self.hidden_size_2 = hidden_size_2\n","        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n","        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.hidden_layer_3 = nn.Linear(hidden_size_2, hidden_size_3)\n","        self.hidden_layer_4 = nn.Linear(hidden_size_3, hidden_size_4)\n","        self.output_layer = nn.Linear(hidden_size_4, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(-1, self.input_size)\n","\n","        h1 = self.activation_fn(self.hidden_layer_1(x))\n","        h2 = self.activation_fn(self.hidden_layer_2(h1))\n","        h3 = self.activation_fn(self.hidden_layer_3(h2))\n","        h4 = self.activation_fn(self.hidden_layer_4(h3))\n","        out = self.output_layer(h4)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# DO NOT MODIFY MODEL INSTANTIATION BELOW\n","#########################################\n","model = MLP(\n","    input_size=784, \n","    hidden_size_1=16384,\n","    hidden_size_2=16384,\n","    hidden_size_3=16384,\n","    hidden_size_4=16384,\n","    activation_fn=nn.ReLU(), \n","    device=device\n",")\n","#########################################\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","num_batches = len(train_dataloader)\n","for epoch in range(20):\n","    for idx, (batch_images, batch_labels) in enumerate(train_dataloader):\n","        if idx % 50 == 0:\n","            print(\"epoch %d, batch %d/%d\" % (epoch, idx, num_batches))\n","\n","        # move labels to GPU (if available)\n","        batch_labels=batch_labels.to(device)\n","        \n","        # batch_size x 2\n","        # feedforward\n","        out = model(batch_images)\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","\n","        # issue:\n","        # loss is a Tensor, which contains the whole computational graph\n","        # of the model. When adding it to the epoch_loss counter, the \n","        # computational graph of each training loop is accumulated in the\n","        # epoch_loss variable, instead of being discarded. Thus, the memory\n","        # usage continues to increase until the out of memory error appears.\n","        # you can also see this issue discussed here:\n","        # https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory\n","        # -\n","        # epoch_loss += loss\n","\n","        # solution:\n","        # extract the scalar from the loss Tensor with .item()\n","        # since the loss variable is not further used, the memory required \n","        # for the computational graph is freed after every iteration\n","        epoch_loss += loss.item()\n","\n","    epoch_loss /= num_batches\n","    print(\"epoch loss = \", epoch_loss)"],"metadata":{"id":"mAahmqrJae1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 9: Underground\n","\n","Validation accuracy on CIFAR10 with this simple MLP should reach ~48%. However, there is a bug preventing that from happening.\n","\n","**Hint**: Inspect the training and validation losses."],"metadata":{"id":"gRKPGcNrSk0Y"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable,\n","                 output_activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.output_activation_fn = output_activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","        \n","        # reshape tensor\n","        # batch_size x 784\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","        out = self.output_activation_fn(out)\n","\n","        return out\n","\n","BATCH_SIZE=128\n","NUM_EPOCHS=20\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# issue 1: NLLLoss (negative log-likelihood loss) in PyTorch assumes that \n","# log probabilities have already been computed. See documentation here:\n","# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss)\n","# Therefore, network outputs should pass through a logSoftMax() activation \n","# function instead of a SoftMax. This is why SoftMax + NLLLoss actually \n","# leads to negative loss values below.\n","# \n","# issue 2: The softmax is computer over dimension 0 (batch dimension) instead\n","# of over dimension 1 (classes). This leads to further performance decrease\n","# model = MLP(\n","#     input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","#     output_activation_fn=nn.Softmax(dim=0)\n","# )\n","model = MLP(\n","    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","    output_activation_fn=nn.LogSoftmax(dim=1)\n",")\n","model.to(device)\n","\n","# instantiate MNIST train and validation datasets\n","train_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=True, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","val_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=False, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","# instantiate dataloaders\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=4\n",")\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=4\n",")\n","num_train_batches = len(train_dataloader)\n","num_val_batches = len(val_dataloader)\n","\n","epoch_loss = 0.0\n","train_losses, val_losses = [], []\n","train_predictions, val_predictions = [], []\n","train_labels, val_labels = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","# DO NOT MODIFY LOSS FUNCTION BELOW\n","##############################################################################\n","loss_crt = nn.NLLLoss()\n","##############################################################################\n","\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","for epoch_idx in range(NUM_EPOCHS):\n","    train_epoch_loss = 0.0\n","    model.train()\n","    for batch_images, batch_labels in train_dataloader:\n","        model.zero_grad()\n","        batch_labels = batch_labels.to(device)\n","        \n","        # feedforward\n","        # batch_size x 10\n","        out = model(batch_images)\n","        \n","        batch_predictions = torch.argmax(out, dim=1)\n","        train_predictions += batch_predictions.tolist()\n","        train_labels += batch_labels.tolist()\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_epoch_loss += loss.item()\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        val_epoch_loss = 0.0\n","        for batch_images, batch_labels in val_dataloader:\n","            batch_labels = batch_labels.to(device)\n","            \n","            # batch_size x 10\n","            # feedforward\n","            out = model(batch_images)\n","            batch_predictions = torch.argmax(out, dim=1)\n","            val_predictions += batch_predictions.tolist()\n","            val_labels += batch_labels.tolist()\n","            \n","            # compute loss \n","            loss = loss_crt(out, batch_labels)\n","            val_epoch_loss += loss.item()\n","    \n","    train_epoch_loss /= num_train_batches\n","    val_epoch_loss /= num_val_batches\n","    train_losses.append(train_epoch_loss)\n","    val_losses.append(val_epoch_loss)\n","    \n","    train_acc = accuracy_score(train_labels, train_predictions)\n","    val_acc = accuracy_score(val_labels, val_predictions)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","    print(\"epoch %d, train loss=%f, val loss=%f, train acc=%f, val acc=%f\" % (\n","        epoch_idx, \n","        train_epoch_loss,\n","        val_epoch_loss,\n","        train_acc,\n","        val_acc\n","    ))\n","    "],"metadata":{"id":"e7S03Z1pSlSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n","plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n","plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracies')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"4IEltdtYSoE3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 10: Validation >> Train \n","\n","Train the network and inspect the train and validation accuracy curves. Notice a large gap in accuracy (>5%) throughout the epochs. You have adjust the code below such that:\n"," - the accuracy gap between train and validation becomes smaller\n"," - validation performance gets better"],"metadata":{"id":"r0qVnHkU_bmd"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable,\n","                 dropout_rate: float):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","        \n","        # reshape tensor\n","        # batch_size x 784\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","\n","        h = self.activation_fn(self.dropout(self.hidden_layer(x)))\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=128\n","NUM_EPOCHS=20\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# issue: dropout rate was set too high, affecting the learning performance on\n","# the training set. The validation performance was higher than training, due to\n","# dropout being disabled during evaluation\n","# model = MLP(\n","#     input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","#     dropout_rate=0.9\n","# )\n","\n","# solution: set smaller dropout rate\n","model = MLP(\n","    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","    dropout_rate=0.2\n",")\n","model.to(device)\n","\n","# instantiate MNIST train and validation datasets\n","train_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=True, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","val_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=False, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","# instantiate dataloaders\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=4\n",")\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=4\n",")\n","num_train_batches = len(train_dataloader)\n","num_val_batches = len(val_dataloader)\n","\n","epoch_loss = 0.0\n","train_losses, val_losses = [], []\n","train_predictions, val_predictions = [], []\n","train_labels, val_labels = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","# DO NOT MODIFY LOSS FUNCTION BELOW\n","##############################################################################\n","loss_crt = nn.CrossEntropyLoss()\n","##############################################################################\n","\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","for epoch_idx in range(NUM_EPOCHS):\n","    train_epoch_loss = 0.0\n","    model.train()\n","    for batch_images, batch_labels in train_dataloader:\n","        model.zero_grad()\n","        batch_labels = batch_labels.to(device)\n","        \n","        # feedforward\n","        # batch_size x 10\n","        out = model(batch_images)\n","        \n","        batch_predictions = torch.argmax(out, dim=1)\n","        train_predictions += batch_predictions.tolist()\n","        train_labels += batch_labels.tolist()\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_epoch_loss += loss.item()\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        val_epoch_loss = 0.0\n","        for batch_images, batch_labels in val_dataloader:\n","            batch_labels = batch_labels.to(device)\n","            \n","            # batch_size x 10\n","            # feedforward\n","            out = model(batch_images)\n","            batch_predictions = torch.argmax(out, dim=1)\n","            val_predictions += batch_predictions.tolist()\n","            val_labels += batch_labels.tolist()\n","            \n","            # compute loss \n","            loss = loss_crt(out, batch_labels)\n","            val_epoch_loss += loss.item()\n","    \n","    train_epoch_loss /= num_train_batches\n","    val_epoch_loss /= num_val_batches\n","    train_losses.append(train_epoch_loss)\n","    val_losses.append(val_epoch_loss)\n","    \n","    train_acc = accuracy_score(train_labels, train_predictions)\n","    val_acc = accuracy_score(val_labels, val_predictions)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n","        epoch_idx, \n","        train_acc,\n","        val_acc\n","    ))\n","    "],"metadata":{"id":"qtzVyNwI_dsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n","plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n","plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracies')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"V_zGvsMxAAQv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyOuAIaV5qT6VO6yYq0P8wX9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"82ee47d85da04df6bbec5bc2dbbc0f83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eb23187ee7f84a8caf9de227f3c2a5c7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7791e791e71a4624abc0ee5f3b993837","IPY_MODEL_37e20b1658cf471eb5e9e3eb60d8f714","IPY_MODEL_fe9857e31d144161ab3dfa0da694cfb2"]}},"eb23187ee7f84a8caf9de227f3c2a5c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7791e791e71a4624abc0ee5f3b993837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70242058f5f445958cfc2f5cc97f9fc2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_539d613253f046d9ba3d5a6cca4e7336"}},"37e20b1658cf471eb5e9e3eb60d8f714":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_78c4fcd456d043cabaf5cbbe2759162d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c250ff8c5f6741f7b8aeee35bb875ed0"}},"fe9857e31d144161ab3dfa0da694cfb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0d6cff1d27b24792807bf5ddd1b2b98a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 19619169.62it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5b5b1b3d8df54e93b536623d8ba108b2"}},"70242058f5f445958cfc2f5cc97f9fc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"539d613253f046d9ba3d5a6cca4e7336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"78c4fcd456d043cabaf5cbbe2759162d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c250ff8c5f6741f7b8aeee35bb875ed0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d6cff1d27b24792807bf5ddd1b2b98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5b5b1b3d8df54e93b536623d8ba108b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5625556804e1402ea35bad12a50602a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b79f4a06afae43bfb8d52b8164550dcf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f693688ab84e4409864468718e1f65ea","IPY_MODEL_1fcad33212074d7190cee61b0acf06a1","IPY_MODEL_2932938c95c14112bac48bd31b2cb20a"]}},"b79f4a06afae43bfb8d52b8164550dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f693688ab84e4409864468718e1f65ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_072664088ed34e049ccc45b1543235a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_46f5a980615c4cd388def319f7f3a721"}},"1fcad33212074d7190cee61b0acf06a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8d2ccc5e585c42db99e35f967fb3ea10","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b107775747e4ebfb7b2e7f17817631e"}},"2932938c95c14112bac48bd31b2cb20a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b805a7f4aae64af4acea9567d2508e8a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 6894.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc81171d82814f8cb5447878eb0ff5e2"}},"072664088ed34e049ccc45b1543235a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"46f5a980615c4cd388def319f7f3a721":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d2ccc5e585c42db99e35f967fb3ea10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8b107775747e4ebfb7b2e7f17817631e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b805a7f4aae64af4acea9567d2508e8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dc81171d82814f8cb5447878eb0ff5e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8883d141ffe74da8b8c13c1e58047513":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_355b3351b29d4961a4e5cdaacf262734","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1e483e2e124040fc8a035ea3179b189f","IPY_MODEL_a783b52295e2472e85da514318004f59","IPY_MODEL_8d71cb6a6c334edaaed30e4314db24f1"]}},"355b3351b29d4961a4e5cdaacf262734":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e483e2e124040fc8a035ea3179b189f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a394c7b2e9d54998b47008230cca4e19","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_00721e39470c4c87843c62304fcf5eaa"}},"a783b52295e2472e85da514318004f59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fda2cb5f22654485ae6825284bb502da","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e4bb6fe52b94972a4e764cd3cba2861"}},"8d71cb6a6c334edaaed30e4314db24f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb79ebc20ff3436ab81285dd53bf0f7d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:00&lt;00:00, 1109734.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2394553a493485198b4b25e4dbdaf41"}},"a394c7b2e9d54998b47008230cca4e19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"00721e39470c4c87843c62304fcf5eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fda2cb5f22654485ae6825284bb502da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3e4bb6fe52b94972a4e764cd3cba2861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb79ebc20ff3436ab81285dd53bf0f7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c2394553a493485198b4b25e4dbdaf41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96e2e4cbff4f4b8ba62f8353f2a9c6f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b23c1c008bbf41f59e0e5605132fe900","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9a14590276dc4c5ca20dbceaacccbd72","IPY_MODEL_e00bb6d2dc014523a1df7681cd07dbc1","IPY_MODEL_a1918aae919e4493bf4880368a56e5ff"]}},"b23c1c008bbf41f59e0e5605132fe900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a14590276dc4c5ca20dbceaacccbd72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_40b9c5938cf84c728b746e90602fd7e8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8671ca14c40e43fe85e76733243cf809"}},"e00bb6d2dc014523a1df7681cd07dbc1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_79048debe5f040978bdd7d3a95a36478","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac3308818c5842d4887aa7e0db7cac7f"}},"a1918aae919e4493bf4880368a56e5ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f7be7a221d0a463abb1498227798797f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:00&lt;00:00, 7625.90it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33d595b780c34d358cf151b4c33a46f6"}},"40b9c5938cf84c728b746e90602fd7e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8671ca14c40e43fe85e76733243cf809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79048debe5f040978bdd7d3a95a36478":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ac3308818c5842d4887aa7e0db7cac7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7be7a221d0a463abb1498227798797f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"33d595b780c34d358cf151b4c33a46f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}