{"cells":[{"cell_type":"markdown","metadata":{"id":"NjZTLbz4Vx4c"},"source":["<font size=25>Laboratory 4 summary</font>\n","\n","In this lab you will gain debugging experience by solving the most typical deep learning bugs. \n","\n","There are 10 exercises, each one with a corresponding cell. Run the cell, inspect the error and fix the code. \n","\n","Tips:\n"," - the bugs can be fixed by editing one or two lines of code  \n"," - some code in the sections must not be modified and is clearly delimited with comments\n"," - try not to inspect other exercises while solving the current one"]},{"cell_type":"markdown","metadata":{"id":"HUoo-4E12opk"},"source":["# **Exercises**\n","\n","Run the cell below to import the packages, which are required for all the exercises below."]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import os\n","import torch\n","import random\n","from typing import Iterator, List, Callable, Tuple\n","from functools import partial\n","import warnings\n","from math import *\n","import zipfile\n","from tqdm import tqdm\n","from PIL import Image\n","\n","# Sklearn\n","from sklearn.datasets import load_digits\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import MinMaxScaler\n","# Numpy\n","import numpy as np\n","# Pandas\n","import pandas as pd\n","\n","# PyTorch packages\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.utils.data import RandomSampler, Sampler\n","from torchvision import transforms, utils, datasets\n","from torchvision.transforms import ToTensor, ToPILImage\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# matplotlib\n","from matplotlib import rc, cm\n","rc('animation', html='jshtml')\n","import matplotlib.pyplot as plt\n","from mpl_toolkits import mplot3d\n","import matplotlib.animation as animation\n","%matplotlib notebook\n","#warnings.filterwarnings(\"ignore\")\n","plt.ion()   # interactive mode"],"metadata":{"id":"8vvjpuXSFmFD","executionInfo":{"status":"ok","timestamp":1678872290374,"user_tz":-120,"elapsed":5000,"user":{"displayName":"Florin Brad","userId":"06472899283008656976"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6db64a78-65c9-469a-86e9-f74ba02a0d0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.pyplot._IonContext at 0x7f7bf3945b50>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"BsINHO_NwDv5"},"source":["## Exercise 1: Getting started\n"]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY MODEL INSTANTIATION BELOW\n","#####################################################################\n","model = MLP(input_size=100, hidden_size=256, activation_fn=nn.ReLU())\n","#####################################################################\n","\n","x = torch.rand(32, 200)\n","\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"k1BvB8TwEPzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Getting in shape"],"metadata":{"id":"uHW1o4myBc4m"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(2*hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY MODEL INSTANTIATION BELOW\n","#####################################################################\n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n","#####################################################################\n","\n","# download MNIST dataset\n","mnist_trainset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","\n","# select 10th example\n","x, l = mnist_trainset[10]\n","\n","y = model(x)\n","\n","assert y.shape[0] == 1 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"3GCJ9iNHNFIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3: It's the little things\n"],"metadata":{"id":"3beV3q_MBf4u"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(x)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU)\n","\n","x = torch.rand(32, 784)\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"ijj1R6SXP-fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: No one left behind"],"metadata":{"id":"QAJtebM_Bjck"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.batch_size = batch_size\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        # input x has shape: batch_size x 1 x 28 x 28 \n","        # we resize it to:   batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY HYPERPARAMETERS AND MODEL INSTANTIATION BELOW\n","#############################################################\n","BATCH_SIZE=32\n","model = MLP(\n","    input_size=784, \n","    hidden_size=256, \n","    activation_fn=nn.ReLU(), \n","    batch_size=BATCH_SIZE\n",")\n","##############################################################\n","\n","# instantiate MNIST dataset\n","val_dataset = datasets.MNIST(\n","    root='./data', \n","    train=False, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in val_dataloader:\n","    # batch_size x 2\n","    out = model(batch_images)\n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(val_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"id":"cbXjL4t37_JZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 5: Left to their own devices\n"],"metadata":{"id":"BvVmqB1tHSF4"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","        self.batch_size = batch_size\n","\n","    def forward(self, x):\n","        # move Tensor to GPU (if available)\n","        x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# DO NOT MODIFY DEVICE TENSOR BELOW\n","#################################################################################\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","#################################################################################\n","\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32,\n","    device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation, it recursively \n","# processes parameters inside your nn.Module)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    # move labels to GPU (if available)\n","    batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"bymjthPlMBY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 6: Not exactly my type"],"metadata":{"id":"6xEcE46Y9q74"}},{"cell_type":"markdown","source":["### Task I"],"metadata":{"id":"ixW0tkSJ9dTu"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.batch_size = batch_size\n","\n","    def forward(self, x):\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32\n",")\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"uqhsW2M3-B1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task II"],"metadata":{"id":"OSXSMtDM-Pz3"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 64\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=64, hidden_size=256, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# load the 1797 images from the Digits dataset:\n","# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n","# Images are grayscale digits from 0 to 9, stored as arrays of size 64 (8x8). \n","# Both images and labels are stored are NumPy arrays, so we need to convert \n","# them to Tensors.\n","x = load_digits()\n","\n","# 1797 x 64, 1797\n","images, labels = torch.tensor(x.data), torch.tensor(x.target)\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (1797 in our case)\n","train_dataset = TensorDataset(images, labels)\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"WDdeFJtSW0ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Out of bounds\n","The [Wheat Seeds](https://archive.ics.uci.edu/ml/datasets/seeds) dataset ([Kaggle link](https://www.kaggle.com/jmcaro/wheat-seedsuci)) is a classification task with 3 classes, which contains 209 examples. Each example contains 7 geometrical properties of wheat seeds belonging to 3 varieties of wheat. \n","\n","**Hint 1:** When training on GPUs, CUDA errors may be less helpful. Usually, errors such as \"`RuntimeError: CUDA error: device-side assert triggered`\" indicate a problem with an index, which may be too large. To get a more accurate error message, move the model and dataset to CPU, check the error again and try to fix it.\n","\n","**Hint 2:** After fixing the code responsible for a CUDA error, you may still encounter the error when running on GPU. Try restarting the Colab Notebook (`Runtime` -> `Restart runtime`) and run the cells again.\n"],"metadata":{"id":"1WRHhMa4nX8R"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 3)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 7\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","\n","# if you encounter a vague CUDA error message, move the operations to CPU then\n","# run the code again. The error message is usually more helpful.\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# download Wheat Seeds dataset\n","!wget --no-check-certificate \\\n","https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n","-O /tmp/wheat.csv\n","\n","# read Wheat Seeds dataset from csv\n","# Dataset has 209 examples. Each example has 7 attributes (features).\n","# It's a classification task with 3 classes (1, 2 and 3)\n","data = pd.read_csv(\"/tmp/wheat.csv\")\n","\n","# put examples in a Tensor\n","x = torch.tensor(data.values, dtype=torch.float32)\n","\n","# separate data and labels\n","data, labels = x[:,:-1], x[:,-1].long()\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (209 in our case)\n","validation_dataset = TensorDataset(data, labels)\n","\n","# instantiate dataloader\n","validation_dataloader = DataLoader(\n","    validation_dataset,\n","    batch_size=BATCH_SIZE\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in validation_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # feedforward\n","    # batch_size x 3\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(validation_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"id":"xZP5yCbeo5Pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 8: I have no memory of that\n","\n","**Hint 1:** The error will appear after ~1 epoch\n","\n","**Hint 2:** You do NOT need to modify the model's size to fix the memory bug\n","\n","**Hint 3:** After getting the error message, you have to restart the machine:\n","  - restart Colab: `Runtime` -> `Restart runtime`\n","  - run the cell that imports packages \n","  - run the cell below"],"metadata":{"id":"4v5D0k3MHsW0"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size_1: int, \n","                 hidden_size_2: int, \n","                 hidden_size_3: int, \n","                 hidden_size_4: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size_1 = hidden_size_1\n","        self.hidden_size_2 = hidden_size_2\n","        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n","        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.hidden_layer_3 = nn.Linear(hidden_size_2, hidden_size_3)\n","        self.hidden_layer_4 = nn.Linear(hidden_size_3, hidden_size_4)\n","        self.output_layer = nn.Linear(hidden_size_4, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(-1, self.input_size)\n","\n","        h1 = self.activation_fn(self.hidden_layer_1(x))\n","        h2 = self.activation_fn(self.hidden_layer_2(h1))\n","        h3 = self.activation_fn(self.hidden_layer_3(h2))\n","        h4 = self.activation_fn(self.hidden_layer_4(h3))\n","        out = self.output_layer(h4)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# DO NOT MODIFY MODEL INSTANTIATION BELOW\n","#########################################\n","model = MLP(\n","    input_size=784, \n","    hidden_size_1=16384,\n","    hidden_size_2=16384,\n","    hidden_size_3=16384,\n","    hidden_size_4=16384,\n","    activation_fn=nn.ReLU(), \n","    device=device\n",")\n","#########################################\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","num_batches = len(train_dataloader)\n","for epoch in range(20):\n","    for idx, (batch_images, batch_labels) in enumerate(train_dataloader):\n","        if idx % 50 == 0:\n","            print(\"epoch %d, batch %d/%d\" % (epoch, idx, num_batches))\n","\n","        # move labels to GPU (if available)\n","        batch_labels=batch_labels.to(device)\n","        \n","        # batch_size x 2\n","        # feedforward\n","        out = model(batch_images)\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","\n","        epoch_loss += loss\n","\n","    epoch_loss /= num_batches\n","    print(\"epoch loss = \", epoch_loss)"],"metadata":{"id":"mAahmqrJae1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 9: Underground\n","\n","Validation accuracy on CIFAR10 with this simple MLP should reach ~48%. However, there is a bug preventing that from happening.\n","\n","**Hint**: Inspect the training and validation losses."],"metadata":{"id":"o_XMAAIA9r0u"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable,\n","                 output_activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.output_activation_fn = output_activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","        \n","        # reshape tensor\n","        # batch_size x 784\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","        out = self.output_activation_fn(out)\n","\n","        return out\n","\n","BATCH_SIZE=128\n","NUM_EPOCHS=20\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","model = MLP(\n","    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","    output_activation_fn=nn.Softmax(dim=0)\n",")\n","model.to(device)\n","\n","# instantiate MNIST train and validation datasets\n","train_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=True, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","val_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=False, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","# instantiate dataloaders\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=4\n",")\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=4\n",")\n","num_train_batches = len(train_dataloader)\n","num_val_batches = len(val_dataloader)\n","\n","epoch_loss = 0.0\n","train_losses, val_losses = [], []\n","train_predictions, val_predictions = [], []\n","train_labels, val_labels = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","# DO NOT MODIFY LOSS FUNCTION BELOW\n","##############################################################################\n","loss_crt = nn.NLLLoss()\n","##############################################################################\n","\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","for epoch_idx in range(NUM_EPOCHS):\n","    train_epoch_loss = 0.0\n","    model.train()\n","    for batch_images, batch_labels in train_dataloader:\n","        model.zero_grad()\n","        batch_labels = batch_labels.to(device)\n","        \n","        # feedforward\n","        # batch_size x 10\n","        out = model(batch_images)\n","        \n","        batch_predictions = torch.argmax(out, dim=1)\n","        train_predictions += batch_predictions.tolist()\n","        train_labels += batch_labels.tolist()\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_epoch_loss += loss.item()\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        val_epoch_loss = 0.0\n","        for batch_images, batch_labels in val_dataloader:\n","            batch_labels = batch_labels.to(device)\n","            \n","            # batch_size x 10\n","            # feedforward\n","            out = model(batch_images)\n","            batch_predictions = torch.argmax(out, dim=1)\n","            val_predictions += batch_predictions.tolist()\n","            val_labels += batch_labels.tolist()\n","            \n","            # compute loss \n","            loss = loss_crt(out, batch_labels)\n","            val_epoch_loss += loss.item()\n","    \n","    train_epoch_loss /= num_train_batches\n","    val_epoch_loss /= num_val_batches\n","    train_losses.append(train_epoch_loss)\n","    val_losses.append(val_epoch_loss)\n","    \n","    train_acc = accuracy_score(train_labels, train_predictions)\n","    val_acc = accuracy_score(val_labels, val_predictions)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n","        epoch_idx, \n","        train_acc,\n","        val_acc\n","    ))\n","    "],"metadata":{"id":"GxeGzxyTUX8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n","plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracies')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"H3hRIIm8zW5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 10: Validation >> Train \n","\n","Train the network and inspect the train and validation accuracy curves. Notice a large gap in accuracy (>5%) throughout the epochs. You have adjust the code below such that:\n"," - the accuracy gap between train and validation becomes smaller\n"," - validation performance gets better"],"metadata":{"id":"LrlATQYJXR0d"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable,\n","                 dropout_rate: float):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","        \n","        # reshape tensor\n","        # batch_size x 784\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","\n","        h = self.activation_fn(self.dropout(self.hidden_layer(x)))\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=128\n","NUM_EPOCHS=20\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","model = MLP(\n","    input_size=3072, hidden_size=1024, activation_fn=nn.ReLU(), device=device,\n","    dropout_rate=0.9\n",")\n","model.to(device)\n","\n","# instantiate MNIST train and validation datasets\n","train_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=True, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","val_dataset = datasets.CIFAR10(\n","    root='./data', \n","    train=False, \n","    download=True,\n","    transform=transforms.ToTensor()\n",")\n","print(\"train dataset size = \", len(train_dataset))\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","# instantiate dataloaders\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=4\n",")\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=4\n",")\n","num_train_batches = len(train_dataloader)\n","num_val_batches = len(val_dataloader)\n","\n","epoch_loss = 0.0\n","train_losses, val_losses = [], []\n","train_predictions, val_predictions = [], []\n","train_labels, val_labels = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","# DO NOT MODIFY LOSS FUNCTION BELOW\n","##############################################################################\n","loss_crt = nn.CrossEntropyLoss()\n","##############################################################################\n","\n","optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","for epoch_idx in range(NUM_EPOCHS):\n","    train_epoch_loss = 0.0\n","    model.train()\n","    for batch_images, batch_labels in train_dataloader:\n","        model.zero_grad()\n","        batch_labels = batch_labels.to(device)\n","        \n","        # feedforward\n","        # batch_size x 10\n","        out = model(batch_images)\n","        \n","        batch_predictions = torch.argmax(out, dim=1)\n","        train_predictions += batch_predictions.tolist()\n","        train_labels += batch_labels.tolist()\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_epoch_loss += loss.item()\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        val_epoch_loss = 0.0\n","        for batch_images, batch_labels in val_dataloader:\n","            batch_labels = batch_labels.to(device)\n","            \n","            # batch_size x 10\n","            # feedforward\n","            out = model(batch_images)\n","            batch_predictions = torch.argmax(out, dim=1)\n","            val_predictions += batch_predictions.tolist()\n","            val_labels += batch_labels.tolist()\n","            \n","            # compute loss \n","            loss = loss_crt(out, batch_labels)\n","            val_epoch_loss += loss.item()\n","    \n","    train_epoch_loss /= num_train_batches\n","    val_epoch_loss /= num_val_batches\n","    train_losses.append(train_epoch_loss)\n","    val_losses.append(val_epoch_loss)\n","    \n","    train_acc = accuracy_score(train_labels, train_predictions)\n","    val_acc = accuracy_score(val_labels, val_predictions)\n","    train_accuracies.append(train_acc)\n","    val_accuracies.append(val_acc)\n","    print(\"epoch %d, train acc=%f, val acc=%f\" % (\n","        epoch_idx, \n","        train_acc,\n","        val_acc\n","    ))\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAenZSaSXUbK","executionInfo":{"status":"ok","timestamp":1678726125106,"user_tz":-120,"elapsed":74080,"user":{"displayName":"Florin Brad","userId":"06472899283008656976"}},"outputId":"b809e02e-ad22-406d-d705-4c4e58201c3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["device =  cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","train dataset size =  50000\n","validation dataset size =  10000\n","epoch 0, train acc=0.180120, val acc=0.281100\n","epoch 1, train acc=0.193400, val acc=0.289700\n","epoch 2, train acc=0.201713, val acc=0.297700\n","epoch 3, train acc=0.206990, val acc=0.302050\n","epoch 4, train acc=0.210948, val acc=0.306620\n","epoch 5, train acc=0.214280, val acc=0.310767\n","epoch 6, train acc=0.217106, val acc=0.314529\n","epoch 7, train acc=0.219908, val acc=0.315775\n","epoch 8, train acc=0.222058, val acc=0.319067\n","epoch 9, train acc=0.223774, val acc=0.321100\n","epoch 10, train acc=0.225705, val acc=0.322645\n","epoch 11, train acc=0.227455, val acc=0.324350\n","epoch 12, train acc=0.228638, val acc=0.326254\n","epoch 13, train acc=0.229809, val acc=0.327800\n","epoch 14, train acc=0.230783, val acc=0.329400\n","epoch 15, train acc=0.231689, val acc=0.330331\n","epoch 16, train acc=0.232551, val acc=0.331365\n","epoch 17, train acc=0.233411, val acc=0.332850\n","epoch 18, train acc=0.234337, val acc=0.333758\n","epoch 19, train acc=0.234991, val acc=0.334865\n"]}]},{"cell_type":"code","source":["%matplotlib inline\n","plt.plot(range(0,len(train_losses)), train_losses, 'g', label='Training loss')\n","plt.plot(range(0,len(train_losses)), val_losses, 'b', label='Validation loss')\n","plt.title('Training and Validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.plot(range(0,len(train_accuracies)), train_accuracies, 'g', label='Training accuracy')\n","plt.plot(range(0,len(train_accuracies)), val_accuracies, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracies')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"UasL94qEZCnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r1qLs0jOl4_r"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}